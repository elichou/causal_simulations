{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing causal bandits and classical bandits algorithms: Simulation on a simple contengency task\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymdp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymdp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymdp'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from typing import Callable, Union, Tuple, Dict, List, Optional\n",
    "import scipy.stats as st\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy.special as special\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable : Union[str, None] = None\n",
    "structural_function : Callable[[Tuple[int, Union[int, None]]], int] \n",
    "structural_equations: Dict[str, Tuple[Callable[[Tuple[int, Union[int, None]]], int], Dict[str, Union[int, None]]]] = {}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Structural Causal Model \n",
    "\n",
    "Structural Causal Model is defined by $<V, F, U, P(U)>$ where $V$ (resp. $U$) is the set of endogenous (resp. exogenous) variables; $F$ is the set of functional equations and $P(U)$ is the probability distribution over the exogenous variables. \n",
    "\n",
    "Methods: \n",
    "\n",
    "- *get_sample*: outputs a single random variate from the internal SCM\n",
    "\n",
    "- *draw*: draw the causal graph associated with the internal SCM\n",
    "\n",
    "- *build_causal_graph*: outputs a causal graph based on the input variable list and structural equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuralCausalModel: \n",
    "    \"\"\"\n",
    "    Structural Causal Model for Causal Bandit implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        variables: List[str], \n",
    "        structural_equations: Dict[str, Tuple[Callable[[Tuple[int, Union[int, None]]], int], Dict[str, Union[int, None]]]]\n",
    "             \n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate StructuralCausalModel class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        variables: list[variable:str]\n",
    "            List containing the name of the variables. \n",
    "\n",
    "        structural_equations: dict[variable: (func, list[variable:str])]\n",
    "            A dictionary containing the structural relations between variables\n",
    "            and their values. \n",
    "        \"\"\"\n",
    "        self.variables = variables # list of variables\n",
    "        self.values = { var: None for var in variables } # list of values taken by each variable\n",
    "        self.structural_equations = structural_equations # functions for each variable\n",
    "        self.causal_graph = self.build_causal_graph(variables, structural_equations) # a causal graph of the SCM\n",
    "\n",
    "    def build_causal_graph(\n",
    "        self, \n",
    "        variables: List[str],\n",
    "        structural_equations: Dict[str, Tuple[Callable[[Tuple[int, Union[int, None]]], int], Dict[str, Union[int, None]]]]\n",
    "        ) -> nx.DiGraph: \n",
    "\n",
    "        \"\"\"\n",
    "        Build a causal graph from variables list and structural equations. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        variables: list[variable]\n",
    "        structural_equations: dict[variable, eqution]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a DiGraph\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        output_graph = nx.DiGraph()\n",
    "        nodes = variables\n",
    "        edges = []\n",
    "        \n",
    "        for variable, equation in structural_equations.items(): \n",
    "            children_node = variable\n",
    "            (function, vars) = equation\n",
    "            \n",
    "            for var, value in vars.items():\n",
    "                parent_node = var\n",
    "                edges.append((parent_node, children_node))\n",
    "\n",
    "        output_graph.add_nodes_from(nodes)\n",
    "        output_graph.add_edges_from(edges)\n",
    "\n",
    "        return output_graph\n",
    "\n",
    "    def graph(self):\n",
    "        \"\"\" Draw the internal causal graph\n",
    "        \"\"\"\n",
    "        nx.draw(self.causal_graph)\n",
    "\n",
    "    def get_sample(self, set_values: Optional[Union[Dict[str, int],None]] = None) -> dict[str, Union[int, None]]:\n",
    "        \"\"\"\n",
    "        Sample from SCM (could be manipulated through set_values).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        set_values: dict[variable, int]\n",
    "            The values fixed by intervention on variables.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        output_assignements : dict[variable, int] \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        output_assignments = {var : None for var in self.variables}\n",
    "\n",
    "        if set_values is not None: \n",
    "            \n",
    "            # Assign values to manipulated variables\n",
    "            for variable, value in set_values.items():\n",
    "                output_assignments[variable] = value\n",
    "                self.values[variable] = value\n",
    "            \n",
    "            # Assign values inside the structural_equations for variables which parents are manipulated variables\n",
    "            for node in self.causal_graph.nodes:\n",
    "                structural_function, parents = self.structural_equations[node]\n",
    "                for parent in parents.keys():\n",
    "                    if parent in set_values.keys():\n",
    "                        parents[parent] = set_values[parent]\n",
    "\n",
    "            # Assign values to remaining variables for output\n",
    "            for node in nx.topological_sort(self.causal_graph):\n",
    "                if node in set_values.keys():\n",
    "                    pass\n",
    "                else:\n",
    "                    structural_function, parents = self.structural_equations[node]\n",
    "                    output_assignments[node] = structural_function(parents)  \n",
    "                    # when a value is assigned, make sure that it is also assigned in the structural_equations \n",
    "                    for node2 in self.causal_graph.nodes:\n",
    "                        structural_function, parents = self.structural_equations[node2]\n",
    "                        if node in parents.keys():\n",
    "                            parents[node] = output_assignments[node]\n",
    "\n",
    "        else:\n",
    "            for node in nx.topological_sort(self.causal_graph):\n",
    "                    structural_function, parents = self.structural_equations[node]\n",
    "                    output_assignments[node] = structural_function(parents)  \n",
    "                    # when a value is assigned, make sure that it is also assigned in the structural_equations \n",
    "                    for node2 in self.causal_graph.nodes:\n",
    "                        structural_function, parents = self.structural_equations[node2]\n",
    "                        if node in parents.keys():\n",
    "                            parents[node] = output_assignments[node]\n",
    "\n",
    "\n",
    "        return output_assignments  \n",
    "\n",
    "        \"\"\"\n",
    "        output_assignments = {}\n",
    "    \n",
    "        if set_values is not None: \n",
    "            \n",
    "            # Assign values to manipulated variables\n",
    "            for variable, value in set_values.items(): \n",
    "                output_assignments[variable] = value\n",
    "                self.values[variable] = value\n",
    "                \n",
    "            # Assign values to manipulated parent variables\n",
    "            for node in self.causal_graph.nodes: \n",
    "                structural_function, parents = self.structural_equations[node]\n",
    "                for parent in parents.keys():\n",
    "                    if parent in set_values.keys():\n",
    "                        parents[parent] = set_values[parent]\n",
    "                        self.values[parent] = set_values[parent]\n",
    "        \n",
    "            # Assign values to remaining variables \n",
    "            for node in nx.topological_sort(self.causal_graph): \n",
    "                if node in set_values.keys():\n",
    "                    continue\n",
    "                structural_function, parents = self.structural_equations[node]\n",
    "                output_assignments[node] = structural_function(parents)\n",
    "   \n",
    "        else:\n",
    "            # Assign values to all variables\n",
    "            for node in nx.topological_sort(self.causal_graph): \n",
    "                structural_function, parents = self.structural_equations[node]\n",
    "                output_assignments[node] = structural_function(parents) \n",
    "\n",
    "        # Assign values inside the equations\n",
    "        for key, value in self.structural_equations.items(): \n",
    "            function, parent_di = value\n",
    "            for key2, value2 in parent_di.items():\n",
    "                parent_di[key2] = self.values[key2]\n",
    "        \n",
    "        return output_assignments\n",
    "        \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Bandit\n",
    "\n",
    "\n",
    "A causal bandit is a bandit where variables are related using a Structural Causal Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalBandit:\n",
    "    \"\"\"\n",
    "    Causal Bandit Class\n",
    "\n",
    "    A causal bandit is a bandit where variables relations are represented on a causal graph/SCM.\n",
    "    In this general case, variables are named X_i.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        structural_causal_model, \n",
    "        number_of_episode = 40,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate Causal Bandit Class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        structural_causal_model: StructuralCausalModel instance\n",
    "            the Structurral Causal Model of the Causal Bandit. \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.scm = structural_causal_model\n",
    "        self.payout_history: List[int] = []\n",
    "        self.counter = 0\n",
    "        self.episode = number_of_episode\n",
    "\n",
    "    def get_sample(self, action):\n",
    "        \"\"\"\n",
    "        Return the payout of a single pull of causal bandit i's arm. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Index of causal bandit to pull.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        int or None\n",
    "        \"\"\"\n",
    "        arm, action_value = action\n",
    "\n",
    "        # sample the scm with the manipulated variables \"arm\" set to \"action_value\"\n",
    "        output = self.scm.get_sample(set_values={arm : action_value})\n",
    "\n",
    "        # save the reward \n",
    "        self.payout_history.append(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def step(self, action: Tuple[str, int]):\n",
    "        \"\"\" Excecute action on env and returns reward, info, observation and done. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : tuple[variable, float]\n",
    "            Action to execute.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        reward: float\n",
    "        info: str\n",
    "        observation: dict[variable, float]\n",
    "        is_done: boolean\n",
    "        \"\"\"\n",
    "\n",
    "        observation = self.get_sample(action)\n",
    "        reward = observation[\"Y\"]\n",
    "        is_done = (self.counter >= self.episode)\n",
    "        info = \"Round n°{} of {}\".format(self.counter, self.episode)\n",
    "\n",
    "        self.counter += 1\n",
    "        \n",
    "        return (observation, reward, is_done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Causal Bandit \n",
    "\n",
    "A Bernoulli Causal bandit is a Bernoulli bandit with variables related using a SCM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliCausalBandit(CausalBandit):\n",
    "    \"\"\" Bernoulli Bandit with two choices: (X=0) or (X=1).   \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, number_of_episode=40):\n",
    "        \n",
    "        \"\"\" Instantiate a Bernoulli Causal Bandit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params: list[float]\n",
    "            List of Bernoulli parameters.\n",
    "\n",
    "            Z |  P(Y = 1 | X = 1, Z)    |   P(Y = 1 | X = 0, Z)\n",
    "            --|-------------------------|----------------------\n",
    "            0 |  params[0]              |   params[1]\n",
    "            1 |  params[2]              |   params[3]\n",
    "\n",
    "\n",
    "\n",
    "            Z ~ P(Z) = Bern(params[4]) \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.counter = 0\n",
    "        self.episode = number_of_episode\n",
    "        self.params: Optional[List[float]] = params # list of Bernoulli parameters\n",
    "        self.variables = [\"X\", \"Y\", \"Z\"] \n",
    "        self.history_best_payout = []\n",
    "        self.regrets = []\n",
    "\n",
    "\n",
    "        def best_arm(params):\n",
    "            \"\"\" Identify the best arm for bernoulli static causal bandit. \n",
    "                pair ou impair et pas supérieur ou inférieur\n",
    "            \"\"\"\n",
    "            index = np.argmax(params)\n",
    "            output = np.mod(index, 2)\n",
    "\n",
    "            return output\n",
    "\n",
    "        self.best_arm = best_arm(params)\n",
    " \n",
    "        def f_X(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for X\n",
    "                not used in practive because X is always manipulated\n",
    "            \"\"\"\n",
    "            if arg[\"Z\"] == 0:\n",
    "                return st.bernoulli.rvs(self.params[5])\n",
    "            else:\n",
    "                return st.bernoulli.rvs(self.params[6])\n",
    "\n",
    "        def f_Y(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Y\n",
    "            \"\"\"\n",
    "            dict_values = {(0, 0): 1, (0, 1): 3, (1, 0): 0, (1, 1) : 2, (0, None): 1, (1, None): 3}\n",
    "            \n",
    "            return st.bernoulli.rvs(self.params[dict_values[(arg[\"X\"], arg[\"Z\"])]])\n",
    "\n",
    "        def f_Z(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Z\n",
    "            \"\"\"\n",
    "            \n",
    "            return st.bernoulli.rvs(self.params[4])\n",
    "\n",
    "        structural_equations = {\n",
    "            \"Z\" : (f_Z, {}),\n",
    "            \"X\" : (f_X, { \"Z\" : None}),\n",
    "            \"Y\" : (f_Y, { \"X\" : None, \"Z\" : None}),\n",
    "        }\n",
    "\n",
    "        scm = StructuralCausalModel(self.variables, structural_equations)\n",
    "\n",
    "        super().__init__(scm)\n",
    "\n",
    "    def get_sample(self, action: float): \n",
    "        \"\"\"\n",
    "        Return the payout of a single pull of causal bandit i's arm. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Index of causal bandit to pull.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        int or None\n",
    "        \"\"\"\n",
    "        if action != {}:\n",
    "            arm, action_value = action\n",
    "            output = self.scm.get_sample(set_values={arm : action_value})\n",
    "        else:\n",
    "            output = self.scm.get_sample({})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.regrets.append(1 - output[\"Y\"])\n",
    "        self.payout_history.append(output)\n",
    "\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def step(self, action: tuple[variable, float]):\n",
    "        \"\"\" Excecute action on env and returns reward, info, observation and done. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : tuple[variable, float]\n",
    "            Action to execute.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        reward: float\n",
    "        info: str\n",
    "        observation: dict[variable, float]\n",
    "        done: boolean\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        observation = self.get_sample(action)\n",
    "        reward = observation[\"Y\"]\n",
    "        is_done = (self.counter >= self.episode)\n",
    "        info = \"Round n°{}\".format(self.counter)\n",
    "\n",
    "        self.counter += 1\n",
    "        \n",
    "        return (observation, reward, is_done, info)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset the observation and the counter of the environment\n",
    "        \"\"\"\n",
    "        self.counter = 0\n",
    "        self.regrets = []\n",
    "        \n",
    "        return {var : 0 for var in self.variables}\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'BernoulliCausalBandit'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Non-stationnary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliChangingCausalBandit(CausalBandit):\n",
    "    \"\"\" Bernoulli Bandit with two choices: (X=0) or (X=1).\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, number_of_episode=40):\n",
    "        \n",
    "        \"\"\" Instantiate a Bernoulli Causal Bandit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        params: list[float]\n",
    "            List of Bernoulli parameters.\n",
    "            Z |  P(W|P,Z)\n",
    "            --------------------\n",
    "            0 |  params[0]\n",
    "            1 |  params[2]\n",
    "\n",
    "            Z ~ P(Z) = Bern(params[4])\n",
    "\n",
    "            X ~ if Z = 0:\n",
    "                    Bern(params[5])\n",
    "                else:\n",
    "                    Bern(params[6])\n",
    " \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.counter = 0\n",
    "        self.episode = number_of_episode\n",
    "        self.params: Optional[list[float]] = params\n",
    "        self.variables = [\"X\", \"Y\", \"Z\"] \n",
    "        self.best_hist_payouts = []\n",
    "        self.regrets = []\n",
    "        \n",
    "\n",
    "\n",
    "        def best_arm(params):\n",
    "            \"\"\" Identify the best arm for bernoulli statitc causal bandit. \n",
    "                pair ou impair et pas supérieur ou inférieur\n",
    "            \"\"\"\n",
    "\n",
    "            index = np.argmax(params)\n",
    "            output = np.mod(index, 2)\n",
    "\n",
    "            return output\n",
    "\n",
    "        self.best_arm = best_arm(params)\n",
    " \n",
    "        def f_X(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for X\n",
    "                not used in practive because X is always manipulated\n",
    "            \"\"\"\n",
    "            if arg[\"Z\"] == 0:\n",
    "                return st.bernoulli.rvs(self.params[5])\n",
    "            else:\n",
    "                return st.bernoulli.rvs(self.params[6])           \n",
    "        \n",
    "        def f_Y(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Y\n",
    "            \"\"\"\n",
    "\n",
    "            dict_values = {(0, 0): 1, (0, 1): 0, (1, 0): 3, (1, 1) : 2, (None, 0): 1, (None, 1): 3}\n",
    "            \n",
    "            return st.bernoulli.rvs(self.params[dict_values[(arg[\"Z\"], arg[\"X\"])]])\n",
    "\n",
    "\n",
    "        def f_Z(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Z\n",
    "            \"\"\"\n",
    "            return st.bernoulli.rvs(self.params[4])\n",
    "\n",
    "        structural_equations = {\n",
    "            \"Z\" : (f_Z, {}),\n",
    "            \"X\" : (f_X, { \"Z\" : None}),\n",
    "            \"Y\" : (f_Y, { \"X\" : None, \"Z\" : None}),\n",
    "        }\n",
    "        self.structural_equations = structural_equations\n",
    "\n",
    "        scm = StructuralCausalModel(self.variables, structural_equations)\n",
    "\n",
    "        super().__init__(scm)\n",
    "\n",
    "    def get_sample(self, action): \n",
    "        \"\"\"\n",
    "        Return the payout of a single pull of causal bandit i's arm. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Index of causal bandit to pull.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        int or None\n",
    "        \"\"\"\n",
    "        arm, action_value = action\n",
    "        output = self.scm.get_sample(set_values={arm : action_value})\n",
    "\n",
    "        best_output = 1\n",
    "\n",
    "        self.regrets.append(best_output - output[\"Y\"])\n",
    "        self.payout_history.append(output)\n",
    "\n",
    "        return output  \n",
    "\n",
    "    def step(self, action: tuple[variable, float]):\n",
    "        \"\"\" Excecute action on env and returns reward, info, observation and done. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : tuple[variable, float]\n",
    "            Action to execute.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        reward: float\n",
    "        info: str\n",
    "        observation: dict[variable, float]\n",
    "        done: boolean\n",
    "        \"\"\"\n",
    "        observation = self.get_sample(action)\n",
    "        reward = observation[\"Y\"]\n",
    "        is_done = (self.counter >= self.episode)\n",
    "        info = \"Round n°{}\".format(self.counter)\n",
    "\n",
    "        self.counter += 1\n",
    "        \n",
    "        return (observation, reward, is_done, info)\n",
    "\n",
    "    def reset(self, switch:float=0.0):\n",
    "        \"\"\" Reset the observation and the counter of the environment\n",
    "        \"\"\"\n",
    "        self.counter = 0\n",
    "        self.regrets = []\n",
    "\n",
    "\n",
    "        def new_f_Z(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Z\n",
    "            \"\"\"\n",
    "            return st.bernoulli.rvs(switch)\n",
    "\n",
    "        self.structural_equations[\"Z\"] = (new_f_Z, {})\n",
    "        new_scm = StructuralCausalModel(self.variables, self.structural_equations)\n",
    "        self.scm = new_scm\n",
    "\n",
    "        return {var : 0 for var in self.variables }\n",
    "\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'BernoulliChangingCausalBandit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bandit\n",
    "\n",
    "A Bernoulli bandit is a classical multi-armed bandit. Arms are only responsible for the outcome and not for other arms respsonse. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit(CausalBandit):\n",
    "    \"\"\" Bernoulli Bandit with two choices: (X=0) or (X=1).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params=[0.3,0.6,0.6,0.8], episode=40):\n",
    "        \"\"\" Instantiate a Bernoulli Causal Bandit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        params: list[float]\n",
    "            List of Bernoulli parameters.\n",
    "            X |  P(Y|X)\n",
    "            --------------------\n",
    "            0 |  params[0]\n",
    "            1 |  params[1]\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.episode = episode\n",
    "        self.params: Optional[list[float]] = params\n",
    "        self.variables = [\"X\", \"Y\"] \n",
    "        self.best_hist_payouts = []\n",
    "        self.regrets = []\n",
    "\n",
    "        def best_arm(params):\n",
    "            \"\"\" Identify the best arm for bernoulli statitc causal bandit. \n",
    "            \n",
    "            \"\"\"\n",
    "            index = np.argmax(params)\n",
    "            output = np.mod(index, 2)\n",
    "            return output\n",
    "\n",
    "        self.best_arm = best_arm(params)\n",
    " \n",
    "        def f_X(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for X\n",
    "            \"\"\"\n",
    "            output = st.bernoulli.rvs(0.5)\n",
    "            return output\n",
    "        \n",
    "        def f_Y(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Y\n",
    "            \"\"\"\n",
    "            output = 0\n",
    "            x_value = arg[\"X\"]\n",
    "            if x_value == 0:\n",
    "                output = st.bernoulli.rvs(self.params[0])\n",
    "            else:\n",
    "                output = st.bernoulli.rvs(self.params[1])\n",
    "            return output\n",
    "\n",
    "        structural_equations = {\n",
    "            \"X\" : (f_X, {}),\n",
    "            \"Y\" : (f_Y, { \"X\" : None}),\n",
    "        }\n",
    "\n",
    "        scm = StructuralCausalModel(self.variables, structural_equations)\n",
    "        super().__init__(scm)\n",
    "\n",
    "    def pull(self, i: float): \n",
    "        \"\"\"\n",
    "        Return the payout of a single pull of causal bandit i's arm. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Index of causal bandit to pull.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        int or None\n",
    "        \"\"\"\n",
    "        output = self.scm.get_sample(set_values={\"X\" : i})\n",
    "        #best_output = self.scm.sample(set_values = {\"X\" : self.best_arm})\n",
    "        best_output = 1\n",
    "        self.regrets.append(best_output - output[\"Y\"])\n",
    "        self.payout_history.append(output)\n",
    "        return output  \n",
    "\n",
    "    def step(self, action: tuple[variable, float]):\n",
    "        \"\"\" Excecute action on env and returns reward, info, observation and done. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : tuple[variable, float]\n",
    "            Action to execute.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        reward: float\n",
    "        info: str\n",
    "        observation: dict[variable, float]\n",
    "        done: boolean\n",
    "        \"\"\"\n",
    "        var, value = action \n",
    "\n",
    "        observation = self.pull(value)\n",
    "        reward = observation[\"Y\"]\n",
    "        done = (self.counter >= self.episode)\n",
    "        info = \"Round n°{}\".format(self.counter)\n",
    "        self.counter += 1\n",
    "        return (observation, reward, done, info)\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        observation = {}\n",
    "        for var in self.variables:\n",
    "            observation[var] = 0\n",
    "        self.regrets = []\n",
    "        return observation\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'BernoulliBandit'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Generic class for agent model implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: list[variable, float]):\n",
    "        \"\"\"\n",
    "        Instantiate a model for decision making on causal bandit envs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actions : list[variable, float]\n",
    "            list of possible actions. \n",
    "        \n",
    "        \"\"\"\n",
    "        self.actions = actions\n",
    "\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        \"\"\" default is random sample. \n",
    "        \"\"\"\n",
    "        action = random.choices(self.actions)\n",
    "        return action[0]\n",
    "\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'RandomAgent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson Sampling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSAgent(Agent):\n",
    "    \"\"\" Thompson Sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, actions: list[variable, float]):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(actions)\n",
    "        self.beta_params = [(1, 1) for k in range(self.K)]\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "\n",
    "        # update beta params\n",
    "        index = self.actions.index((\"X\", observation[\"X\"]))\n",
    "        alpha, beta = self.beta_params[index]\n",
    "        self.beta_params[index] = (alpha + observation[\"Y\"], beta + 1 - observation[\"Y\"])\n",
    "\n",
    "        # sample model\n",
    "        theta_est = []\n",
    "        for k in range(self.K):\n",
    "            theta_est.append(st.beta.rvs(self.beta_params[k][0], self.beta_params[k][1]))\n",
    "\n",
    "        # select action\n",
    "        action_index = np.argmax(theta_est)\n",
    "        return self.actions[action_index]\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'TSAgent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Causal Thompson Sampling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCTSAgent(Agent):\n",
    "    \"\"\"\n",
    "    OCTS agent without causal graph knowledge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: list[variable, float]):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(self.actions)\n",
    "        self.n_part = 4\n",
    "        self.dirc = np.ones([self.n_part, self.K], dtype=int)\n",
    "        self.beta = np.ones([self.n_part, 2], dtype=int)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        \"\"\" Select action based on OCTS algorithm\n",
    "            Note that X is always manipulated here\n",
    "            so this does not work for an observational setting.\n",
    "        \"\"\"\n",
    "\n",
    "        # update\n",
    "        list_observation = [(key, value) for key, value in observation.items()]\n",
    "        z = sum([2**i * list_observation[i][1] for i in range(len(list_observation)-1)])\n",
    "        self.dirc[z, int(observation[\"X\"])] += 1\n",
    "        self.beta[z, 1 - int(observation[\"Y\"])] += 1\n",
    "\n",
    "        # sample        \n",
    "        success_chance = np.zeros(self.K)\n",
    "\n",
    "        for a in range(self.K):\n",
    "            partition_prob = np.random.dirichlet(self.dirc[:,a]).reshape(-1,1)\n",
    "            sample_prob = np.random.beta(self.beta[:, 0], self.beta[:, 1]).reshape(-1,1)\n",
    "            success_chance[a] = sum(np.matmul(sample_prob.T, partition_prob))[0]\n",
    "            \n",
    "        # select action\n",
    "        action_index = np.argmax(success_chance)\n",
    "        \n",
    "        return self.actions[action_index]\n",
    "\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'OCTSAgent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCTSAgent_with_graph(Agent):\n",
    "    \"\"\"\n",
    "    OCTS agent without causal graph knowledge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: list[variable, float], env):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(self.actions)\n",
    "        self.n_part = 4\n",
    "        self.beta = np.ones([self.n_part, self.K], dtype=int)\n",
    "        self.env = env\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "\n",
    "        # update\n",
    "        list_observation = [(key, value) for key, value in observation.items()]\n",
    "        z = sum([2**i * list_observation[i][1] for i in range(len(list_observation)-1)])\n",
    "        \n",
    "\n",
    "        # sample        \n",
    "        success_chance = np.zeros(self.K)\n",
    "\n",
    "        for a in range(self.K):\n",
    "            partition_prob = np.array([[st.bernoulli.rvs(self.env.params[4]), st.bernoulli.rvs(1-self.env.params[4])]]).reshape(-1, 1)\n",
    "            sample_prob = np.random.beta(self.beta[:,0], self.beta[:, 1]).reshape(-1,1)\n",
    "            success_chance[a] = sum(np.matmul(sample_prob, partition_prob.T))[0]\n",
    "            print(\"success_chance:{}\".format(success_chance))\n",
    "\n",
    "        # select action\n",
    "        action_index = np.argmax(success_chance)\n",
    "\n",
    "        return self.actions[action_index]\n",
    "\n",
    "\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'OCTSAgentWithGraph'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Inference Truncated Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "class AIAgent(Agent):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    def __init__(self, actions: list[variable, float]):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(actions)\n",
    "        self.beta_params = [(1, 1) for k in range(self.K)]\n",
    "        self.precision = 1.0\n",
    "        \n",
    "    def choose_action(self, observation, previous_action) : #, tmp = []):\n",
    "\n",
    "        # update beta params\n",
    "        var, value = previous_action\n",
    "        index = self.actions.index((var, value)) # corriger erreur ici\n",
    "        alpha_x, beta_x = self.beta_params[index]\n",
    "        self.beta_params[index] = (alpha_x + observation[\"Y\"], beta_x + 1 - observation[\"Y\"])\n",
    "\n",
    "        # compute approximate free energy\n",
    "        est_eff = []\n",
    "        tmp2 = []\n",
    "        for k in range(self.K):\n",
    "            alpha, beta = self.beta_params[k]\n",
    "            nu = alpha + beta\n",
    "            mu = alpha/nu\n",
    "            est_eff.append(2*self.precision*mu + 1/(2*nu))\n",
    "            tmp2.append(2*self.precision*mu + 1/(2*nu))\n",
    "\n",
    "        #tmp.append(scipy.special.softmax(tmp2)) \n",
    "        #print(\"g:{}\".format(est_eff))\n",
    "        # select action\n",
    "        action_index = np.argmax(est_eff)\n",
    "\n",
    "        return self.actions[action_index] #, tmp\n",
    "\n",
    "\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'AIAgent'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Inference Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangle(x):\n",
    "    output = None\n",
    "    if x < 1/2:\n",
    "        output = 2*x\n",
    "    else:\n",
    "        output = -2*x + 2\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIBanditAgent(Agent):\n",
    "\n",
    "    def __init__(self, actions: list[variable, float], param: float, precision:float = 0.1):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(actions)\n",
    "        self.beta_params = [(1, 1) for k in range(self.K)]  \n",
    "        self.precision = precision\n",
    "        self.rho = triangle(param)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "\n",
    "        # update beta params\n",
    "        index = self.actions.index((\"X\", observation[\"X\"]))\n",
    "        alpha_x, beta_x = self.beta_params[index]\n",
    "        self.beta_params[index] = (alpha_x + observation[\"Y\"], beta_x + 1 - observation[\"Y\"])\n",
    "        \n",
    "        # compute approximate free energy\n",
    "        g = []\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            alpha, beta = self.beta_params[k]\n",
    "            nu_1 = alpha + beta\n",
    "            mu_1 = alpha/nu_1\n",
    "            mu = mu_1 + self.rho*(0.5 - mu_1)\n",
    "\n",
    "            g_a = -2*self.precision*(1-self.rho)*mu_1 + mu*np.log(mu) + (1-mu)*np.log(1 - mu) - \\\n",
    "                 (1 - self.rho)*(mu_1*special.digamma(alpha) + 1 - mu_1*special.digamma(beta)) + \\\n",
    "                    (1 - self.rho)*(special.digamma(nu_1)-1/nu_1) + 1\n",
    "            g.append(g_a)\n",
    "        \n",
    "        # select action\n",
    "        action_index = np.argmax(g)\n",
    "        \n",
    "        return self.actions[action_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations and Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(name):\n",
    "    if name == \"AIAgent\":\n",
    "        print('ok')\n",
    "    else:\n",
    "        print(\"notok\")\n",
    "\n",
    "f(\"AIAgent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probs(rewards, actions):\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "         \n",
    "\n",
    "        # compute probability to win when the agent's play\n",
    "        win_play = [((action[1] and reward)) for (action, reward) in zip(actions, rewards)]\n",
    "        num_win_play = [np.sum(win_play[:i]) for i in range(len(win_play))]\n",
    "        num_play = [np.sum([action[1] for action in actions[:i]]) for i in range(len(actions))]\n",
    "        p_win_play = [win_play_t/play_total_t for (win_play_t, play_total_t) in zip(num_win_play, num_play)]\n",
    "\n",
    "        # compute the probability to win when the agent doesn't play\n",
    "        win_no_play = [int(not(action[1]) and reward) for (action, reward) in zip(actions, rewards)]\n",
    "        num_win_no_play = [np.sum(win_no_play[:i]) for i in range(len(win_no_play))]\n",
    "        num_no_play = [np.sum([int(not(action[1])) for action in actions[:i]]) for i in range(len(actions))]\n",
    "        p_win_no_play = [win_no_play_t/no_play_t for (win_no_play_t, no_play_t) in zip(num_win_no_play, num_no_play)]\n",
    "\n",
    "        # compute probability of play\n",
    "        p_win_play = [(0.5 if np.isnan(p) else p) for p in p_win_play]\n",
    "        p_win_no_play = [(0.5 if np.isnan(p) else p) for p in p_win_no_play]\n",
    "\n",
    "        p_play = [num_play/i for (i, num_play) in enumerate(num_play)]\n",
    "\n",
    "        return p_win_play, p_win_no_play, p_play\n",
    "\n",
    "def plot_prob(agent_name: str, env_name: str, actions, switch: float=0.0, reset_agent: bool=True, change: bool = False):\n",
    "\n",
    "    \n",
    "    index = [\"0.6\", \"0.4\", \"0.0\", \"-0.4\"]\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20,5))\n",
    "    fig.suptitle(agent_name)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")   \n",
    "\n",
    "        for k in range(4):\n",
    "            list_params = [[0.8, 0.2, 0.7, 0.3, switch],\n",
    "                        [0.7, 0.3, 0.7, 0.3, switch],\n",
    "                        [0.5, 0.5, 0.5, 0.5, switch],\n",
    "                        [0.3, 0.7, 0.3, 0.7, switch]\n",
    "                        ]\n",
    "            \n",
    "            if str(env_name).find('BernoulliCausalBandit') > 0:\n",
    "                list_params = [[0.8, 0.2, 0.7, 0.3, 0.5, 0.5, switch],\n",
    "                        [0.7, 0.3, 0.7, 0.3, 0.5, 0.5, switch],\n",
    "                        [0.5, 0.5, 0.5, 0.5,0.5, 0.5, switch],\n",
    "                        [0.3, 0.7, 0.3, 0.7,0.5, 0.5, switch]\n",
    "                        ]\n",
    "                        \n",
    "            PARAMS = list_params[k]\n",
    "            env = env_name(PARAMS)\n",
    "            observation = {'X': 0, 'Y': 0, 'Z': 0}\n",
    "        \n",
    "            if not(reset_agent):\n",
    "                agent = agent_name(actions)\n",
    "\n",
    "            list_pwp = []\n",
    "            list_pwnp = []\n",
    "            list_play = []\n",
    "\n",
    "            for i in tqdm(range(100)):\n",
    "\n",
    "                hist_observations = []\n",
    "                hist_randag_rewards = []\n",
    "                hist_randag_actions = []\n",
    "                observation = env.reset()\n",
    "\n",
    "                if reset_agent:\n",
    "                    agent = agent_name(actions)\n",
    "\n",
    "                previous_action = agent.actions[np.random.randint(2)]\n",
    "\n",
    "                for t in range(40):\n",
    "                    if ((t == 20) and change):\n",
    "                        observation = env.reset()\n",
    "\n",
    "\n",
    "                    if (str(agent).find(\"AIAgent\") > 0 ):\n",
    "                        action = agent.choose_action(observation, previous_action)\n",
    "                        previous_action = action\n",
    "                    else:\n",
    "                        action = agent.choose_action(observation)\n",
    "                        \n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    hist_randag_rewards.append(reward)\n",
    "                    hist_randag_actions.append(action)\n",
    "                    \n",
    "                    #if done:\n",
    "                    #     print(\"End of this episode.\")\n",
    "                    #    break\n",
    "                    \n",
    "\n",
    "                randag_pwp, randag_pwnp, p_play = compute_probs(hist_randag_rewards, hist_randag_actions)\n",
    "                list_pwp.append(randag_pwp)\n",
    "                list_pwnp.append(randag_pwnp)\n",
    "                list_play.append(p_play)\n",
    "\n",
    "\n",
    "            list_pwp = np.asarray(list_pwp)\n",
    "            _, m = np.shape(list_pwp)\n",
    "            \n",
    "            pwp_means = np.asarray(([np.nanmean(list_pwp[:,j]) for j in range(m)]))\n",
    "            pwp_stds = np.asarray(([np.nanstd(list_pwp[:,j]) for j in range(m)]))\n",
    "\n",
    "            x = np.arange(m) \n",
    "            \n",
    "            axs[0].plot(x, pwp_means, label=\"P(win|play)={}\".format(PARAMS[0]))\n",
    "            axs[0].fill_between(x, pwp_means-pwp_stds, pwp_means+pwp_stds, alpha = 0.2, )\n",
    "            axs[0].set_title('P(win|play)')\n",
    "            axs[0].legend()\n",
    "\n",
    "            list_pwnp = np.asarray(list_pwnp)\n",
    "            _, m = np.shape(list_pwnp)\n",
    "            \n",
    "            pwnp_means = np.asarray(([np.nanmean(list_pwnp[:,j]) for j in range(m)]))\n",
    "            pwnp_stds = np.asarray(([np.nanstd(list_pwnp[:,j]) for j in range(m)]))\n",
    "\n",
    "            pwnp_means = np.nan_to_num(pwnp_means)\n",
    "            pwnp_stds = np.nan_to_num(pwnp_stds)\n",
    "            \n",
    "            x = np.arange(m)\n",
    "\n",
    "            axs[1].plot(x, pwnp_means, label=\"P(win|not play)={}\".format(PARAMS[1]))\n",
    "            axs[1].fill_between(x, pwnp_means-pwnp_stds, pwnp_means+pwnp_stds, alpha = 0.2)\n",
    "            axs[1].set_title('P(win|not play)')\n",
    "            axs[1].legend()\n",
    "\n",
    "\n",
    "            dp_means = np.nan_to_num(pwp_means - pwnp_means)\n",
    "            dp_stds = np.nan_to_num(pwp_stds - pwnp_stds)\n",
    "\n",
    "            axs[2].plot(x, dp_means,  label=\"dP={}\".format(np.round(PARAMS[0]-PARAMS[1], 4)))\n",
    "            axs[2].fill_between(x, dp_means-dp_stds, dp_means+dp_stds, alpha=0.2)\n",
    "            axs[2].set_title(\"dP\")\n",
    "            axs[2].legend()\n",
    "\n",
    "            list_play = np.asarray(list_play)\n",
    "            _, m = np.shape(list_play)\n",
    "            p_play_mean = np.asarray(([np.nanmean(list_play[:, j]) for j in range(m)]))\n",
    "            p_play_std = np.asarray(([np.nanstd(list_play[:, j]) for j in range(m)]))\n",
    "            mean = np.nanmean(p_play_mean)\n",
    "            std = np.nanmean(p_play_std)\n",
    "            axs[3].bar(index[k], (mean), yerr=std)\n",
    "            axs[3].set_title(\"P(Play)\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between real dP and estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_causal_estimates(actions, env, params):\n",
    "\n",
    "    agents = [AIAgent, TSAgent, OCTSAgent, Agent]\n",
    "    envs = [env(params) for i in range(len(agents))]\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(8,5))\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")   \n",
    "\n",
    "        for k in range(len(agents)):\n",
    "\n",
    "            current_env = envs[k]\n",
    "\n",
    "            observation = {'X': 0, 'Y': 0, 'Z': 0}\n",
    "\n",
    "            list_pwp = []\n",
    "            list_pwnp = []\n",
    "            list_play = []\n",
    "\n",
    "            for i in tqdm(range(100)):\n",
    "\n",
    "                current_agent = agents[k](actions)\n",
    "                \n",
    "                hist_observations = []\n",
    "                hist_randag_rewards = []\n",
    "                hist_randag_actions = []\n",
    "                observation = current_env.reset()\n",
    "\n",
    "                previous_action = current_agent.actions[np.random.randint(len(current_agent.actions))]\n",
    "\n",
    "                for t in range(40):\n",
    "\n",
    "                    if (str(current_agent).find(\"AIAgent\") > 0 ):\n",
    "                        action = current_agent.choose_action(observation, previous_action)\n",
    "                        previous_action = action\n",
    "                    else:\n",
    "                        action = current_agent.choose_action(observation)\n",
    "                        \n",
    "                    observation, reward, done, info = current_env.step(action)\n",
    "                    hist_randag_rewards.append(reward)\n",
    "                    hist_randag_actions.append(action)\n",
    "                    \n",
    "\n",
    "                randag_pwp, randag_pwnp, p_play = compute_probs(hist_randag_rewards, hist_randag_actions)\n",
    "                list_pwp.append(randag_pwp)\n",
    "                list_pwnp.append(randag_pwnp)\n",
    "                list_play.append(p_play)\n",
    "\n",
    "            list_pwp = np.asarray(list_pwp)\n",
    "            list_pwnp = np.asarray(list_pwnp)\n",
    "            _, m = np.shape(list_pwnp)\n",
    "            _, m = np.shape(list_pwp)\n",
    "            \n",
    "            pwp_means = np.asarray(([np.nanmean(list_pwp[:,j]) for j in range(m)]))\n",
    "            pwp_stds = np.asarray(([np.nanstd(list_pwp[:,j]) for j in range(m)]))\n",
    "           \n",
    "            pwnp_means = np.asarray(([np.nanmean(list_pwnp[:,j]) for j in range(m)]))\n",
    "            pwnp_stds = np.asarray(([np.nanstd(list_pwnp[:,j]) for j in range(m)]))\n",
    "\n",
    "            pwnp_means = np.nan_to_num(pwnp_means)\n",
    "            pwnp_stds = np.nan_to_num(pwnp_stds)\n",
    "\n",
    "            dp_means = np.nan_to_num(pwp_means - pwnp_means)\n",
    "            dp_stds = np.nan_to_num(pwp_stds - pwnp_stds)\n",
    "\n",
    "            x = np.arange(m)\n",
    "\n",
    "            axs.plot(x, dp_means,  label=\"{}\".format(current_agent.__name__()))\n",
    "            axs.fill_between(x, dp_means-dp_stds, dp_means+dp_stds, alpha=0.2)\n",
    "\n",
    "            #if current_env.__name__() == \"BernoulliCausalBandit\"\n",
    "            axs.plot(x, ((params[1]-params[0]))*np.ones(np.shape(x)))\n",
    "            axs.set_title(\"dP\")\n",
    "            axs.legend()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = [0.2,0.8,0.3,0.7]\n",
    "PARAMS = [0.2, 0.8, 0.3, 0.7, 0.5, 0.5, 0.5]\n",
    "\n",
    "ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "ACTIONS = [(\"X\",0),(\"X\",1), (\"Z\", 0), (\"Z\", 1)]\n",
    "\n",
    "#ENV = BernoulliCausalBandit\n",
    "ENV = BernoulliCausalBandit\n",
    "plot_comparison_causal_estimates(ACTIONS, ENV, PARAMS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 : Bernoulli standard bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_regrets_bandit():\n",
    "    PARAMS = [0.6,0.4]\n",
    "    ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "\n",
    "    agents = [AIAgent(ACTIONS), TSAgent(ACTIONS), OCTSAgent(ACTIONS), Agent(ACTIONS)]#, AIBanditAgent(ACTIONS, 1.0)]\n",
    "    envs = [BernoulliBandit(PARAMS) for i in range(len(agents))]\n",
    "    regrets = [[] for i in range(len(agents))]\n",
    "\n",
    "    for i in tqdm(range(len(agents))): \n",
    "        current_agent = agents[i]\n",
    "        current_env = envs[i]\n",
    "        current_regret = regrets[i]\n",
    "\n",
    "        previous_action = ACTIONS[np.random.randint(2)]\n",
    "\n",
    "        for t in range(1000):\n",
    "            observation = current_env.reset()\n",
    "            for k in range(40):\n",
    "                if (i==0):\n",
    "                    action = current_agent.choose_action(observation, previous_action)\n",
    "                    previous_action = action\n",
    "                else:\n",
    "                    action = current_agent.choose_action(observation)\n",
    "                observation, reward, done, info = current_env.step(action)\n",
    "            current_regret.append(current_env.regrets)\n",
    "            \n",
    "        current_cumul_reg = np.cumsum(current_regret, axis=0)\n",
    "        _,m = np.shape(current_cumul_reg)\n",
    "        means = np.asarray([np.mean(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        stds = np.asarray([np.std(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        x = np.arange(m)\n",
    "\n",
    "        plt.plot(x, means)\n",
    "        plt.fill_between(x, means - stds, means + stds, alpha = 0.2, label='_nolegend_')\n",
    "        #plt.xticks(x)\n",
    "    plt.legend(['Active Inference',  'TS', 'OCTS',  'random'])\n",
    "    plt.title('Cumulative Regrets - Bernoulli Bandit')\n",
    "\n",
    "plot_cumulative_regrets_bandit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist_rewards_actions(agent, env):\n",
    "\n",
    "    observation = env.reset()\n",
    "\n",
    "    hist_observations = []\n",
    "    hist_rewards = []\n",
    "    hist_actions = []\n",
    "\n",
    "    previous_action = agent.actions[np.random.randint(2)]\n",
    "\n",
    "    for t in range(40):\n",
    "\n",
    "        if ('AIAgent' in str(agent)):\n",
    "            action = agent.choose_action(observation, previous_action)\n",
    "            previous_action = action\n",
    "        else:\n",
    "            action = agent.choose_action(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        hist_rewards.append(reward)\n",
    "        hist_actions.append(action)\n",
    "    \n",
    "\n",
    "    return hist_rewards, hist_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = [0.2,0.8,0.2,0.8, 1.0]\n",
    "ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "\n",
    "env = BernoulliBandit(PARAMS)\n",
    "\n",
    "randagent = Agent(ACTIONS)\n",
    "aiagent = AIAgent(ACTIONS)\n",
    "aibanditgent = AIBanditAgent(ACTIONS, param = PARAMS[4])\n",
    "tsagent = TSAgent(ACTIONS)\n",
    "octsagent = OCTSAgent(ACTIONS)\n",
    "\n",
    "hist_randag_rewards, hist_randag_actions = get_hist_rewards_actions(randagent, env)\n",
    "hist_tsag_rewards, hist_tsag_actions = get_hist_rewards_actions(tsagent, env)\n",
    "hist_octsag_rewards, hist_octsag_actions = get_hist_rewards_actions(octsagent, env)\n",
    "hist_aiag_rewards, hist_aiag_actions = get_hist_rewards_actions(aiagent, env)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 : Bernoulli Causal Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_regrets_causal_bandit():\n",
    "    PARAMS = [0.9,0.1,0.6,0.4, 0.5]\n",
    "    ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "\n",
    "    agents = [AIAgent(ACTIONS), TSAgent(ACTIONS), OCTSAgent(ACTIONS), Agent(ACTIONS)]#, AIBanditAgent(ACTIONS, PARAMS[4])]\n",
    "    envs = [BernoulliCausalBandit(PARAMS) for i in range(len(agents))]\n",
    "    regrets = [[] for i in range(len(agents))]\n",
    "\n",
    "    for i in tqdm(range(len(agents))): \n",
    "        current_agent = agents[i]\n",
    "        current_env = envs[i]\n",
    "        current_regret = regrets[i]\n",
    "\n",
    "        previous_action = ACTIONS[np.random.randint(2)]\n",
    "\n",
    "        for t in range(1000):\n",
    "            observation = current_env.reset()\n",
    "            for k in range(40):\n",
    "                if (i==0):\n",
    "                    action = current_agent.choose_action(observation, previous_action)\n",
    "                    previous_action = action\n",
    "                else:\n",
    "                    action = current_agent.choose_action(observation)\n",
    "                observation, reward, done, info = current_env.step(action)\n",
    "            current_regret.append(current_env.regrets)\n",
    "        \n",
    "        current_cumul_reg = np.cumsum(current_regret, axis=0)\n",
    "        _,m = np.shape(current_cumul_reg)\n",
    "        means = np.asarray([np.mean(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        stds = np.asarray([np.std(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        x = np.arange(m)\n",
    "\n",
    "        plt.plot(x, means)\n",
    "        plt.fill_between(x, means - stds, means + stds, alpha = 0.2, label='_nolegend_')\n",
    "\n",
    "    plt.legend(['Active Inference',  'TS', 'OCTS',  'random', 'AI Bandit'])\n",
    "    plt.title('Cumulative Regrets - Causal Bandit')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_cumulative_regrets_causal_bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randag_pwp, randag_pwnp, _ = compute_probs(hist_randag_rewards, hist_randag_actions)\n",
    "tsag_pwp, tsag_pwnp, _ = compute_probs(hist_tsag_rewards, hist_tsag_actions)\n",
    "octsag_pwp, octsag_pwnp, _ = compute_probs(hist_octsag_rewards, hist_octsag_actions)\n",
    "aiag_pwp, aiag_pwnp, _ = compute_probs(hist_aiag_rewards, hist_aiag_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "plot_prob(Agent, BernoulliBandit, ACTIONS,  switch=0.8, reset_agent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TS Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prob(TSAgent, BernoulliBandit, actions= ACTIONS,  reset_agent=True, switch=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCTS agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prob(OCTSAgent, BernoulliCausalBandit, ACTIONS,  reset_agent=True, switch=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active Inference agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prob(AIAgent, BernoulliCausalBandit, ACTIONS, reset_agent=True, switch=0.4)\n",
    "#agent = AIBanditAgent(ACTIONS, PARAMS[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p(play) over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probs_conf(arg_rewards, arg_actions):\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        actions = [action for action in arg_actions if action[0] == \"X\"]\n",
    "        rewards = [reward for (action, reward) in zip(arg_actions, arg_rewards) if action[0] == \"X\"]\n",
    "\n",
    "        # compute probability to win when the agent's play\n",
    "        win_play = [((action[1] and reward)) for (action, reward) in zip(actions, rewards)]\n",
    "        num_win_play = [np.sum(win_play[:i]) for i in range(len(win_play))]\n",
    "        num_play = [np.sum([action[1] for action in actions[:i]]) for i in range(len(actions))]\n",
    "        p_win_play = [win_play_t/play_total_t for (win_play_t, play_total_t) in zip(num_win_play, num_play)]\n",
    "\n",
    "        # compute the probability to win when the agent doesn't play\n",
    "        win_no_play = [int(not(action[1]) and reward) for (action, reward) in zip(actions, rewards)]\n",
    "        num_win_no_play = [np.sum(win_no_play[:i]) for i in range(len(win_no_play))]\n",
    "        num_no_play = [np.sum([int(not(action[1])) for action in actions[:i]]) for i in range(len(actions))]\n",
    "        p_win_no_play = [win_no_play_t/no_play_t for (win_no_play_t, no_play_t) in zip(num_win_no_play, num_no_play)]\n",
    "\n",
    "        # compute probability of play\n",
    "        p_win_play = [(0.5 if np.isnan(p) else p) for p in p_win_play]\n",
    "        p_win_no_play = [(0.5 if np.isnan(p) else p) for p in p_win_no_play]\n",
    "\n",
    "        p_play = [num_play/i for (i, num_play) in enumerate(num_play)]\n",
    "\n",
    "        return p_win_play, p_win_no_play, p_play\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_p_play(agent_name, reset_agent=True):\n",
    "\n",
    "    list_params = [[0.2, 0.8, 0.3, 0.7, 0.6, 0.9, 0.1]]\n",
    "                    #[0.7, 0.3, 0.3, 0.7, switch],\n",
    "                    #[0.5, 0.5, 0.5, 0.5, switch],\n",
    "                    #[0.3, 0.7, 0.7, 0.3, switch]]\n",
    "    p_play_mean = []\n",
    "    p_play_std = []\n",
    "    ACTIONS = [(\"X\",0),(\"X\",1), (\"Z\", 0), (\"Z\", 1)]\n",
    "\n",
    "    for k in range(0,len(list_params)):\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "\n",
    "            warnings.simplefilter(\"ignore\") \n",
    "\n",
    "            PARAMS = list_params[k]\n",
    "            env = BernoulliCausalBandit(PARAMS)\n",
    "            observation = env.reset()  \n",
    "\n",
    "            if not(reset_agent):\n",
    "                agent = agent_name(ACTIONS)\n",
    "\n",
    "            list_play = []\n",
    "\n",
    "            for i in tqdm(range(1000)):\n",
    "\n",
    "                hist_observations = []\n",
    "                hist_ag_rewards = []\n",
    "                hist_ag_actions = []\n",
    "\n",
    "                observation = env.reset()\n",
    "\n",
    "                \n",
    "                agent = agent_name(ACTIONS)\n",
    "                    \n",
    "                previous_action = agent.actions[np.random.randint(len(agent.actions))]\n",
    "\n",
    "\n",
    "                for k in range(40):\n",
    "                    action = agent.choose_action(observation)\n",
    "                    observation, reward, done, info = env.step({})\n",
    "                    hist_ag_rewards.append(reward)\n",
    "                    hist_ag_actions.append(action) \n",
    "                    \n",
    "                for k in range(40):    \n",
    "                    action = agent.choose_action(observation)\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    hist_ag_rewards.append(reward)\n",
    "                    hist_ag_actions.append(action) \n",
    "\n",
    "                #for t in range(40):\n",
    "                #    if (t == 20 ):\n",
    "                #        observation = env.reset()\n",
    "\n",
    "                #    if  agent.__name__() == \"AIAgent\":\n",
    "                #        action = agent.choose_action(observation, previous_action)\n",
    "                #        previous_action = action\n",
    "                #    else:\n",
    "                #        action = agent.choose_action(observation)\n",
    "                #    observation, reward, done, info = env.step(action)\n",
    "\n",
    "                #    hist_ag_rewards.append(reward)\n",
    "                #    hist_ag_actions.append(action) \n",
    "\n",
    "\n",
    "                _, _, p_play = compute_probs(hist_ag_rewards, hist_ag_actions)\n",
    "                \n",
    "                list_play.append(p_play)\n",
    "\n",
    "            list_play = np.asarray(list_play)\n",
    "            _, m = np.shape(list_play)\n",
    "            p_play_mean.append(np.asarray(([np.nanmean(list_play[:, j]) for j in range(m)])))\n",
    "            p_play_std.append(np.asarray(([np.nanstd(list_play[:, j]) for j in range(m)])))\n",
    "\n",
    "    return p_play_mean, p_play_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_p_play(Agent))\n",
    "p0, ps0 = compute_p_play(Agent)\n",
    "p1, ps1 = compute_p_play(TSAgent)\n",
    "p2, ps2 = compute_p_play(OCTSAgent)\n",
    "#p3, ps3 = compute_p_play(AIAgent)\n",
    "\n",
    "LIST_PARAMS = [0.2,0.3, 0.5, 0.7]\n",
    "\n",
    "#fig, axs = plt.subplots(1, 4, figsize=(20,5))\n",
    "#m = np.shape(p0)\n",
    "#x = np.arange(m[1])\n",
    "\n",
    "\n",
    "#for k in range(4):\n",
    "#    for j in range(4):\n",
    "#        axs[k].plot(x, locals()[\"p\"+str(k)][j], label=\"P(win|not play)={}\".format(LIST_PARAMS[j]))\n",
    "#        axs[k].fill_between(x, locals()[\"p\"+str(k)][j]-locals()[\"ps\"+str(k)][j], locals()[\"p\"+str(k)][j]+locals()[\"ps\"+str(k)][j], alpha = 0.2)\n",
    "#        axs[k].set_title('P(play)')\n",
    "#        axs[k].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20,5))\n",
    "m = np.shape(p0)\n",
    "x = np.arange(m[1])\n",
    "\n",
    "\n",
    "for k in range(3):\n",
    "    axs[k].plot(x, locals()[\"p\"+str(k)][0], label=\"P(win|not play)={}\".format(0.6))\n",
    "    axs[k].fill_between(x, locals()[\"p\"+str(k)][0]-locals()[\"ps\"+str(k)][0], locals()[\"p\"+str(k)][0]+locals()[\"ps\"+str(k)][0], alpha = 0.2)\n",
    "    axs[k].set_title('P(play)')\n",
    "    axs[k].legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Non-stationary Bernoulli Causal Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_regrets_causal_bandit():\n",
    "    PARAMS = [0.9,0.1,0.6,0.4, 0.5]\n",
    "    ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "\n",
    "    agents = [AIAgent(ACTIONS), TSAgent(ACTIONS), OCTSAgent(ACTIONS), Agent(ACTIONS)]#, AIBanditAgent(ACTIONS, PARAMS[4])]\n",
    "    envs = [BernoulliChangingCausalBandit(PARAMS) for i in range(len(agents))]\n",
    "    regrets = [[] for i in range(len(agents))]\n",
    "\n",
    "    for i in tqdm(range(len(agents))): \n",
    "        current_agent = agents[i]\n",
    "        current_env = envs[i]\n",
    "        current_regret = regrets[i]\n",
    "\n",
    "        previous_action = ACTIONS[np.random.randint(2)]\n",
    "\n",
    "        for t in range(100):\n",
    "            observation = current_env.reset()\n",
    "            for k in range(40):\n",
    "                if (i==0):\n",
    "                    action = current_agent.choose_action(observation, previous_action)\n",
    "                    previous_action = action\n",
    "                else:\n",
    "                    action = current_agent.choose_action(observation)\n",
    "                observation, reward, done, info = current_env.step(action)\n",
    "            current_regret.append(current_env.regrets)\n",
    "        \n",
    "        current_cumul_reg = np.cumsum(current_regret, axis=0)\n",
    "        _,m = np.shape(current_cumul_reg)\n",
    "        means = np.asarray([np.mean(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        stds = np.asarray([np.std(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        x = np.arange(m)\n",
    "\n",
    "        plt.plot(x, means)\n",
    "        plt.fill_between(x, means - stds, means + stds, alpha = 0.2, label='_nolegend_')\n",
    "\n",
    "    plt.legend(['Active Inference',  'TS', 'OCTS',  'random', 'AI Bandit'])\n",
    "    plt.title('Cumulative Regrets - Causal Non-Stationary Bandit')\n",
    "\n",
    "plot_cumulative_regrets_causal_bandit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 : Causal bandit with manipulable confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_regrets_causal_bandit():\n",
    "    PARAMS = [0.9,0.1,0.6,0.4, 0.5, 0.5, 0.6]\n",
    "    ACTIONS = [(\"X\",0),(\"X\",1), (\"Z\", 0), (\"Z\", 1)]\n",
    "\n",
    "    agents = [ TSAgent(ACTIONS), OCTSAgent(ACTIONS), Agent(ACTIONS)]#, AIBanditAgent(ACTIONS, PARAMS[4])]\n",
    "    envs = [BernoulliCausalBandit(PARAMS) for i in range(len(agents))]\n",
    "    regrets = [[] for i in range(len(agents))]\n",
    "\n",
    "    for i in tqdm(range(len(agents))): \n",
    "        current_agent = agents[i]\n",
    "        current_env = envs[i]\n",
    "        current_regret = regrets[i]\n",
    "\n",
    "        previous_action = ACTIONS[np.random.randint(4)]\n",
    "\n",
    "        for t in range(1000):\n",
    "            observation = current_env.reset()\n",
    "            for k in range(40):\n",
    "                if (current_agent.__name__() == \"AIAgent\"):\n",
    "                    action = current_agent.choose_action(observation, previous_action)\n",
    "                    previous_action = action\n",
    "                    \n",
    "                else:\n",
    "                    action = current_agent.choose_action(observation)\n",
    "                observation, reward, done, info = current_env.step(action)\n",
    "            current_regret.append(current_env.regrets)\n",
    "        \n",
    "        current_cumul_reg = np.cumsum(current_regret, axis=0)\n",
    "        _,m = np.shape(current_cumul_reg)\n",
    "        means = np.asarray([np.mean(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        stds = np.asarray([np.std(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        x = np.arange(m)\n",
    "\n",
    "        plt.plot(x, means)\n",
    "        plt.fill_between(x, means - stds, means + stds, alpha = 0.2, label='_nolegend_')\n",
    "\n",
    "    plt.legend([agent.__name__() for agent in agents])\n",
    "    plt.title('Cumulative Regrets - Causal Bandit Manipulable Confounder')\n",
    "\n",
    "plot_cumulative_regrets_causal_bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_regrets_causal_bandit_obs():\n",
    "    PARAMS = [0.9,0.1,0.6,0.4, 0.5, 0.9, 0.1]\n",
    "    ACTIONS = [(\"X\",0),(\"X\",1), (\"Z\", 0), (\"Z\", 1)]\n",
    "\n",
    "    agents = [ TSAgent(ACTIONS), OCTSAgent(ACTIONS), Agent(ACTIONS)]#, AIBanditAgent(ACTIONS, PARAMS[4])]\n",
    "    envs = [BernoulliCausalBandit(PARAMS) for i in range(len(agents))]\n",
    "    regrets = [[] for i in range(len(agents))]\n",
    "\n",
    "    for i in tqdm(range(len(agents))): \n",
    "        current_agent = agents[i]\n",
    "        current_env = envs[i]\n",
    "        current_regret = regrets[i]\n",
    "\n",
    "        previous_action = ACTIONS[np.random.randint(4)]\n",
    "\n",
    "        for t in range(1000):\n",
    "            observation = current_env.reset()\n",
    "\n",
    "            for k in range(20):\n",
    "                action = current_agent.choose_action(observation)\n",
    "                observation, reward, done, info = current_env.step({})\n",
    "            current_regret.append(current_env.regrets)\n",
    "                \n",
    "            for k in range(20):    \n",
    "                action = current_agent.choose_action(observation)\n",
    "                observation, reward, done, info = current_env.step(action)\n",
    "            current_regret.append(current_env.regrets)\n",
    "        \n",
    "        current_cumul_reg = np.cumsum(current_regret, axis=0)\n",
    "        _,m = np.shape(current_cumul_reg)\n",
    "        means = np.asarray([np.mean(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        stds = np.asarray([np.std(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        x = np.arange(m)\n",
    "\n",
    "        plt.plot(x, means)\n",
    "        plt.fill_between(x, means - stds, means + stds, alpha = 0.2, label='_nolegend_')\n",
    "\n",
    "    plt.legend([agent.__name__() for agent in agents])\n",
    "    plt.title('Cumulative Regrets - Causal Bandit Manipulable Confounder')\n",
    "\n",
    "\n",
    "plot_cumulative_regrets_causal_bandit_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_causal_estimates_obs(actions, env, params):\n",
    "\n",
    "    agents = [TSAgent, OCTSAgent, Agent]\n",
    "    envs = [env(params) for i in range(len(agents))]\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(8,5))\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")   \n",
    "\n",
    "        for k in range(len(agents)):\n",
    "\n",
    "            current_env = envs[k]\n",
    "\n",
    "            observation = {'X': 0, 'Y': 0, 'Z': 0}\n",
    "\n",
    "            list_pwp = []\n",
    "            list_pwnp = []\n",
    "            list_play = []\n",
    "\n",
    "            for i in tqdm(range(100)):\n",
    "\n",
    "                current_agent = agents[k](actions)\n",
    "                \n",
    "                hist_observations = []\n",
    "                hist_rewards = []\n",
    "                hist_actions = []\n",
    "                observation = current_env.reset()\n",
    "\n",
    "                previous_action = current_agent.actions[np.random.randint(len(current_agent.actions))]\n",
    "\n",
    "                for t in range(20):\n",
    "                    action = current_agent.choose_action(observation)                        \n",
    "                    observation, reward, done, info = current_env.step({})\n",
    "                    hist_rewards.append(reward)\n",
    "                    hist_actions.append(action) \n",
    "\n",
    "                for t in range(20):\n",
    "\n",
    "                    if (str(current_agent).find(\"AIAgent\") > 0 ):\n",
    "                        action = current_agent.choose_action(observation, previous_action)\n",
    "                        previous_action = action\n",
    "                    else:\n",
    "                        action = current_agent.choose_action(observation)\n",
    "                        \n",
    "                    observation, reward, done, info = current_env.step(action)\n",
    "                    hist_rewards.append(reward)\n",
    "                    hist_actions.append(action)\n",
    "                    \n",
    "\n",
    "                pwp, pwnp, p_play = compute_probs(hist_rewards, hist_actions)\n",
    "                list_pwp.append(pwp)\n",
    "                list_pwnp.append(pwnp)\n",
    "                list_play.append(p_play)\n",
    "\n",
    "            list_pwp = np.asarray(list_pwp)\n",
    "            list_pwnp = np.asarray(list_pwnp)\n",
    "            _, m = np.shape(list_pwnp)\n",
    "            _, m = np.shape(list_pwp)\n",
    "            \n",
    "            pwp_means = np.asarray(([np.nanmean(list_pwp[:,j]) for j in range(m)]))\n",
    "            pwp_stds = np.asarray(([np.nanstd(list_pwp[:,j]) for j in range(m)]))\n",
    "           \n",
    "            pwnp_means = np.asarray(([np.nanmean(list_pwnp[:,j]) for j in range(m)]))\n",
    "            pwnp_stds = np.asarray(([np.nanstd(list_pwnp[:,j]) for j in range(m)]))\n",
    "\n",
    "            pwnp_means = np.nan_to_num(pwnp_means)\n",
    "            pwnp_stds = np.nan_to_num(pwnp_stds)\n",
    "\n",
    "            dp_means = np.nan_to_num(pwp_means - pwnp_means)\n",
    "            dp_stds = np.nan_to_num(pwp_stds - pwnp_stds)\n",
    "\n",
    "            x = np.arange(m)\n",
    "\n",
    "            axs.plot(x, dp_means,  label=\"{}\".format(current_agent.__name__()))\n",
    "            axs.fill_between(x, dp_means-dp_stds, dp_means+dp_stds, alpha=0.2)\n",
    "\n",
    "\n",
    "            #axs.plot(x, ((params[1]-params[0]))*np.ones(np.shape(x)))\n",
    "            axs.set_title(\"dP\")\n",
    "            axs.legend()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMS = [0.2,0.8,0.3,0.7]\n",
    "PARAMS = [0.2, 0.8, 0.3, 0.7, 0.6, 0.9, 0.1]\n",
    "\n",
    "#ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "ACTIONS = [(\"X\",0),(\"X\",1), (\"Z\", 0), (\"Z\", 1)]\n",
    "\n",
    "#ENV = BernoulliCausalBandit\n",
    "ENV = BernoulliCausalBandit\n",
    "plot_comparison_causal_estimates_obs(ACTIONS, ENV, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
