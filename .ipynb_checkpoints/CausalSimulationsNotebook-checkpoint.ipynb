{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations for Causal bandits \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "This notebook presents different sequential decision-making algoritms on two version of causal bandits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Preliminaries\n",
    "\n",
    "    - Structural Causal Model    \n",
    "\n",
    "2. Causal Bandit architecture\n",
    "\n",
    "3. Agent models\n",
    "\n",
    "    - Random Agent\n",
    "    \n",
    "    - Thompson Sampling\n",
    "\n",
    "    - Online Learning Causal Thompson Sampling\n",
    "\n",
    "    - Active inference approximated agent\n",
    "    \n",
    "    - Active inference agent\n",
    "\n",
    "4. Results\n",
    "    \n",
    "5. More plots\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import typing as ty\n",
    "import scipy.stats as st\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymdp\n",
    "from tqdm import tqdm\n",
    "import scipy.special as special\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the types of the variables we will use afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = str\n",
    "structural_function = ty.Callable[[ty.Union[ty.Tuple[str, int]]], int]\n",
    "structural_equations = dict[variable, tuple[structural_function, dict[variable, ty.Union[int, None]]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Structural Causal Model \n",
    "\n",
    "Structural Causal Model is defined by $<V, F, U, P(U)>$ where $V$ (resp. $U$) is the set of endogenous (resp. exogenous) variables; $F$ is the set of functional equations and $P(U)$ is the probability distribution over the exogenous variables. \n",
    "\n",
    "Methods: \n",
    "\n",
    "- *sample*: outputs a single random variate from the internal SCM\n",
    "\n",
    "- *draw*: draw the causal graph associated with the internal SCM\n",
    "\n",
    "- *build_causal_graph*: outputs a causal graph based on the input variable list and structural equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuralCausalModel: \n",
    "    \"\"\"\n",
    "    Structural Causal Model for Causal Bandit implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        variables: list[variable], \n",
    "        structural_equations: structural_equations\n",
    "             \n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate StructuralCausalModel class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        variables: list[variable:str]\n",
    "            List containing the name of the variables. \n",
    "\n",
    "        structural_equations: dict[variable: (func, list[variable:str])]\n",
    "            A dictionary containing the structural relations between variables\n",
    "            and their values. \n",
    "        \"\"\"\n",
    "        self.variables = variables # list of variables\n",
    "        self.values = { var: int for var in variables } # list of values taken by each variable\n",
    "        self.structural_equations = structural_equations # functions for each variable\n",
    "        self.causal_graph = self.build_causal_graph() # a causal graph of the SCM\n",
    "\n",
    "    def build_causal_graph(\n",
    "        self\n",
    "        ) -> nx.DiGraph: \n",
    "\n",
    "        \"\"\"\n",
    "        Build a causal graph from variables list and structural equations. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        variables: list[variable]\n",
    "\n",
    "        structural_equations: dict[variable, eqution]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a DiGraph\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        graph = nx.DiGraph()\n",
    "        \n",
    "        for variable, (function, parents) in self.structural_equations.items():\n",
    "            for parent in parents: \n",
    "                graph.add_edge(parent, variable)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def graph(self):\n",
    "        \"\"\" Draw the internal causal graph\n",
    "            returns nothing\n",
    "        \"\"\"\n",
    "        nx.draw(self.causal_graph)\n",
    "\n",
    "    def sample(self, set_values: typing.Optional[typing.Union[dict[variable, int],None]] = {\"X\" : 0}) -> int:\n",
    "        \"\"\"\n",
    "        Sample from SCM (could be manipulated through set_values).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        set_values: dict[variable, int]\n",
    "            The values fixed by intervention on variables.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "\n",
    "       output : dict[variable, int] \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        output = {}\n",
    "    \n",
    "        if set_values is not None: # set_values is the set of (variables, values) that are manipulated during this round\n",
    "            for variable, value in set_values.items(): # assign values to manipulated variables\n",
    "                output[variable] = value\n",
    "                \n",
    "\n",
    "            for node in self.causal_graph.nodes: # assigns values to manipulated parent variables\n",
    "                structural_function, parents = self.structural_equations[node]\n",
    "                for parent in parents.keys():\n",
    "                    if parent in set_values.keys():\n",
    "                        parents[parent] = set_values[parent]\n",
    "        \n",
    "            for node in nx.topological_sort(self.causal_graph): # assigns values to remaining variables (not manipulated or not parent of manipulated variables)\n",
    "                if node in set_values.keys():\n",
    "                    pass\n",
    "                else:\n",
    "                    structural_function, parents = self.structural_equations[node]\n",
    "                    output[node] = structural_function(parents)\n",
    "              \n",
    "        else:\n",
    "            for node in nx.topological_sort(self.causal_graph): # assigns values to all variables if no variables are manipulated\n",
    "                structural_function, parents = self.structural_equations[node]\n",
    "                output[node] = structural_function(parents) \n",
    "\n",
    "        for var, val in output.items(): # saves the values of this round\n",
    "            self.values[var] = val\n",
    "        \n",
    "        for key, value in self.structural_equations.items(): # also saves the values of this round but this time inside the equations\n",
    "            function, parent_di = value\n",
    "            for key2, value2 in parent_di.items():\n",
    "                parent_di[key2] = self.values[key2]\n",
    "\n",
    "        return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Bandit\n",
    "\n",
    "\n",
    "A causal bandit is a bandit where variables are related using a Structural Causal Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalBandit:\n",
    "    \"\"\"\n",
    "    Causal Bandit Class\n",
    "\n",
    "    A causal bandit is a bandit where variables relations are represented on a causal graph/SCM.\n",
    "    In this general case, variables are named X_i.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        structural_causal_model, \n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate Causal Bandit Class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        structural_causal_model: StructuralCausalModel instance\n",
    "            the Structurral Causal Model of the Causal Bandit. \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.scm = structural_causal_model\n",
    "        self.hist_payouts: list[int] = []\n",
    "        self.counter = 0\n",
    "\n",
    "    def pull(self, i):\n",
    "        \"\"\"\n",
    "        Return the payout of a single pull of causal bandit i's arm. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Index of causal bandit to pull.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        int or None\n",
    "        \"\"\"\n",
    "        output = self.scm.sample(set_values={\"X_{}\".format(i) : 1})\n",
    "        self.hist_payouts.append(output)\n",
    "\n",
    "        return output[\"Y\"]\n",
    "\n",
    "    def step(self, action: tuple[variable, float]):\n",
    "        \"\"\" Excecute action on env and returns reward, info, observation and done. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : tuple[variable, float]\n",
    "            Action to execute.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        reward: float\n",
    "        info: str\n",
    "        observation: dict[variable, float]\n",
    "        done: boolean\n",
    "        \"\"\"\n",
    "        var, value = action \n",
    "        \n",
    "        observation = self.pull(value)\n",
    "        reward = observation[\"Y\"]\n",
    "        done = (self.counter >= self.episode)\n",
    "        info = \"Round n°{}\".format(self.counter)\n",
    "\n",
    "        self.counter += 1\n",
    "        \n",
    "        return (observation, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Causal Bandit \n",
    "\n",
    "A Bernoulli Causal bandit is a Bernoulli bandit with variables related using a SCM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliCausalBandit(CausalBandit):\n",
    "    \"\"\" Bernoulli Bandit with two choices: (X=0) or (X=1).\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, episode=40):\n",
    "        \n",
    "        \"\"\" Instantiate a Bernoulli Causal Bandit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        params: list[float]\n",
    "            List of Bernoulli parameters.\n",
    "            Z |  P(W|P,Z)\n",
    "            --------------------\n",
    "            0 |  params[0]\n",
    "            1 |  params[2]\n",
    "\n",
    "            Z ~ P(Z) = Bern(params[4])\n",
    "\n",
    "            \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.counter = 0\n",
    "        self.episode = episode\n",
    "        self.params: typing.Optional[list[float]] = params\n",
    "        self.variables = [\"X\", \"Y\", \"Z\"] \n",
    "        self.best_hist_payouts = []\n",
    "        self.regrets = []\n",
    "\n",
    "\n",
    "        def best_arm(params):\n",
    "            \"\"\" Identify the best arm for bernoulli statitc causal bandit. \n",
    "                pair ou impair et pas supérieur ou inférieur\n",
    "            \"\"\"\n",
    "\n",
    "            index = np.argmax(params)\n",
    "            output = np.mod(index, 2)\n",
    "\n",
    "            return output\n",
    "\n",
    "        self.best_arm = best_arm(params)\n",
    " \n",
    "        def f_X(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for X\n",
    "                not used in practive because X is always manipulated\n",
    "            \"\"\"\n",
    "\n",
    "            output = 0\n",
    "            z_value = arg[\"Z\"]\n",
    "\n",
    "            if z_value == 0:\n",
    "                output = st.bernoulli.rvs(self.params[5])\n",
    "            else:\n",
    "                output = st.bernoulli.rvs(self.params[6])\n",
    "\n",
    "            return output            \n",
    "        \n",
    "        def f_Y(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Y\n",
    "            \"\"\"\n",
    "\n",
    "            output = 0\n",
    "            x_value = arg[\"X\"]\n",
    "            z_value = arg[\"Z\"]\n",
    "\n",
    "            dict_values = {(0, 0): 1, (0, 1): 0, (1, 0): 3, (1, 1) : 2, (None, 0): 1, (None, 1): 1}\n",
    "            \n",
    "            output = st.bernoulli.rvs(self.params[dict_values[(z_value, x_value)]])\n",
    "\n",
    "            return output\n",
    "\n",
    "        def f_Z(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Z\n",
    "            \"\"\"\n",
    "            return st.bernoulli.rvs(self.params[4])\n",
    "\n",
    "        structural_equations = {\n",
    "            \"Z\" : (f_Z, {}),\n",
    "            \"X\" : (f_X, { \"Z\" : None}),\n",
    "            \"Y\" : (f_Y, { \"X\" : None, \"Z\" : None}),\n",
    "        }\n",
    "\n",
    "        scm = StructuralCausalModel(self.variables, structural_equations)\n",
    "\n",
    "        super().__init__(scm)\n",
    "\n",
    "    def pull(self, i: float): \n",
    "        \"\"\"\n",
    "        Return the payout of a single pull of causal bandit i's arm. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Index of causal bandit to pull.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        int or None\n",
    "        \"\"\"\n",
    "        output = self.scm.sample(set_values={\"X\" : i})\n",
    "\n",
    "        best_output = 1\n",
    "\n",
    "        self.regrets.append(best_output - output[\"Y\"])\n",
    "        self.hist_payouts.append(output)\n",
    "\n",
    "        return output  \n",
    "\n",
    "    def step(self, action: tuple[variable, float]):\n",
    "        \"\"\" Excecute action on env and returns reward, info, observation and done. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : tuple[variable, float]\n",
    "            Action to execute.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        reward: float\n",
    "        info: str\n",
    "        observation: dict[variable, float]\n",
    "        done: boolean\n",
    "        \"\"\"\n",
    "        var, value = action \n",
    "        observation = self.pull(value)\n",
    "        reward = observation[\"Y\"]\n",
    "        done = (self.counter >= self.episode)\n",
    "        info = \"Round n°{}\".format(self.counter)\n",
    "\n",
    "        self.counter += 1\n",
    "        \n",
    "        return (observation, reward, done, info)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset the observation and the counter of the environment\n",
    "        \"\"\"\n",
    "        self.counter = 0\n",
    "        self.regrets = []\n",
    "\n",
    "        observation = {}\n",
    "        \n",
    "        for var in self.variables:\n",
    "            observation[var] = 0\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Changing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliChangingCausalBandit(CausalBandit):\n",
    "    \"\"\" Bernoulli Bandit with two choices: (X=0) or (X=1).\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, episode=40):\n",
    "        \n",
    "        \"\"\" Instantiate a Bernoulli Causal Bandit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        params: list[float]\n",
    "            List of Bernoulli parameters.\n",
    "            Z |  P(W|P,Z)\n",
    "            --------------------\n",
    "            0 |  params[0]\n",
    "            1 |  params[2]\n",
    "\n",
    "            Z ~ P(Z) = Bern(params[4]\n",
    "\n",
    "            \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.counter = 0\n",
    "        self.episode = episode\n",
    "        self.params: typing.Optional[list[float]] = params\n",
    "        self.variables = [\"X\", \"Y\", \"Z\"] \n",
    "        self.best_hist_payouts = []\n",
    "        self.regrets = []\n",
    "        \n",
    "\n",
    "\n",
    "        def best_arm(params):\n",
    "            \"\"\" Identify the best arm for bernoulli statitc causal bandit. \n",
    "                pair ou impair et pas supérieur ou inférieur\n",
    "            \"\"\"\n",
    "\n",
    "            index = np.argmax(params)\n",
    "            output = np.mod(index, 2)\n",
    "\n",
    "            return output\n",
    "\n",
    "        self.best_arm = best_arm(params)\n",
    " \n",
    "        def f_X(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for X\n",
    "                not used in practive because X is always manipulated\n",
    "            \"\"\"\n",
    "\n",
    "            output = 0\n",
    "            z_value = arg[\"Z\"]\n",
    "\n",
    "            if z_value == 0:\n",
    "                output = st.bernoulli.rvs(self.params[5])\n",
    "            else:\n",
    "                output = st.bernoulli.rvs(self.params[6])\n",
    "\n",
    "            return output            \n",
    "        \n",
    "        def f_Y(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Y\n",
    "            \"\"\"\n",
    "\n",
    "            output = 0\n",
    "            x_value = arg[\"X\"]\n",
    "            z_value = arg[\"Z\"]\n",
    "\n",
    "            dict_values = {(0, 0): 1, (0, 1): 0, (1, 0): 3, (1, 1) : 2, (None, 0): 1, (None, 1): 1}\n",
    "            \n",
    "            output = st.bernoulli.rvs(self.params[dict_values[(z_value, x_value)]])\n",
    "\n",
    "            return output\n",
    "\n",
    "        def f_Z(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Z\n",
    "            \"\"\"\n",
    "            return st.bernoulli.rvs(self.params[4])\n",
    "\n",
    "        structural_equations = {\n",
    "            \"Z\" : (f_Z, {}),\n",
    "            \"X\" : (f_X, { \"Z\" : None}),\n",
    "            \"Y\" : (f_Y, { \"X\" : None, \"Z\" : None}),\n",
    "        }\n",
    "        self.structural_equations = structural_equations\n",
    "\n",
    "        scm = StructuralCausalModel(self.variables, structural_equations)\n",
    "\n",
    "        super().__init__(scm)\n",
    "\n",
    "    def pull(self, i: float): \n",
    "        \"\"\"\n",
    "        Return the payout of a single pull of causal bandit i's arm. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Index of causal bandit to pull.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        int or None\n",
    "        \"\"\"\n",
    "        output = self.scm.sample(set_values={\"X\" : i})\n",
    "\n",
    "        best_output = 1\n",
    "\n",
    "        self.regrets.append(best_output - output[\"Y\"])\n",
    "        self.hist_payouts.append(output)\n",
    "\n",
    "        return output  \n",
    "\n",
    "    def step(self, action: tuple[variable, float]):\n",
    "        \"\"\" Excecute action on env and returns reward, info, observation and done. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : tuple[variable, float]\n",
    "            Action to execute.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        reward: float\n",
    "        info: str\n",
    "        observation: dict[variable, float]\n",
    "        done: boolean\n",
    "        \"\"\"\n",
    "        var, value = action \n",
    "        observation = self.pull(value)\n",
    "        reward = observation[\"Y\"]\n",
    "        done = (self.counter >= self.episode)\n",
    "        info = \"Round n°{}\".format(self.counter)\n",
    "\n",
    "        self.counter += 1\n",
    "        \n",
    "        return (observation, reward, done, info)\n",
    "\n",
    "    def reset(self, switch:float=0.0):\n",
    "        \"\"\" Reset the observation and the counter of the environment\n",
    "        \"\"\"\n",
    "        self.counter = 0\n",
    "        self.regrets = []\n",
    "\n",
    "\n",
    "        def new_f_Z(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Z\n",
    "            \"\"\"\n",
    "            return st.bernoulli.rvs(switch)\n",
    "\n",
    "        self.structural_equations[\"Z\"] = (new_f_Z, {})\n",
    "\n",
    "        new_scm = StructuralCausalModel(self.variables, self.structural_equations)\n",
    "\n",
    "        self.scm = new_scm\n",
    "\n",
    "        observation = {}\n",
    "\n",
    "        \n",
    "        for var in self.variables:\n",
    "            observation[var] = 0\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bandit\n",
    "\n",
    "A Bernoulli bandit is a classical multi-armed bandit. Arms are only responsible for the outcome and not for other arms respsonse. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit(CausalBandit):\n",
    "    \"\"\" Bernoulli Bandit with two choices: (X=0) or (X=1).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params=[0.3,0.6,0.6,0.8], episode=40):\n",
    "        \"\"\" Instantiate a Bernoulli Causal Bandit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        params: list[float]\n",
    "            List of Bernoulli parameters.\n",
    "            X |  P(Y|X)\n",
    "            --------------------\n",
    "            0 |  params[0]\n",
    "            1 |  params[1]\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.episode = episode\n",
    "        self.params: typing.Optional[list[float]] = params\n",
    "        self.variables = [\"X\", \"Y\"] \n",
    "        self.best_hist_payouts = []\n",
    "        self.regrets = []\n",
    "\n",
    "        def best_arm(params):\n",
    "            \"\"\" Identify the best arm for bernoulli statitc causal bandit. \n",
    "            \n",
    "            \"\"\"\n",
    "            index = np.argmax(params)\n",
    "            output = np.mod(index, 2)\n",
    "            return output\n",
    "\n",
    "        self.best_arm = best_arm(params)\n",
    " \n",
    "        def f_X(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for X\n",
    "            \"\"\"\n",
    "            output = st.bernoulli.rvs(0.4)\n",
    "            return output\n",
    "        \n",
    "        def f_Y(arg: dict[tuple[variable, int]]) -> int:\n",
    "            \"\"\" function for Y\n",
    "            \"\"\"\n",
    "            output = 0\n",
    "            x_value = arg[\"X\"]\n",
    "            if x_value == 0:\n",
    "                output = st.bernoulli.rvs(self.params[1])\n",
    "            else:\n",
    "                output = st.bernoulli.rvs(self.params[0])\n",
    "            return output\n",
    "\n",
    "        structural_equations = {\n",
    "            \"X\" : (f_X, {}),\n",
    "            \"Y\" : (f_Y, { \"X\" : None}),\n",
    "        }\n",
    "\n",
    "        scm = StructuralCausalModel(self.variables, structural_equations)\n",
    "        super().__init__(scm)\n",
    "\n",
    "    def pull(self, i: float): \n",
    "        \"\"\"\n",
    "        Return the payout of a single pull of causal bandit i's arm. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i: int\n",
    "            Index of causal bandit to pull.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        int or None\n",
    "        \"\"\"\n",
    "        output = self.scm.sample(set_values={\"X\" : i})\n",
    "        #best_output = self.scm.sample(set_values = {\"X\" : self.best_arm})\n",
    "        best_output = 1\n",
    "        self.regrets.append(best_output - output[\"Y\"])\n",
    "        self.hist_payouts.append(output)\n",
    "        return output  \n",
    "\n",
    "    def step(self, action: tuple[variable, float]):\n",
    "        \"\"\" Excecute action on env and returns reward, info, observation and done. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : tuple[variable, float]\n",
    "            Action to execute.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        reward: float\n",
    "        info: str\n",
    "        observation: dict[variable, float]\n",
    "        done: boolean\n",
    "        \"\"\"\n",
    "        var, value = action \n",
    "\n",
    "        observation = self.pull(value)\n",
    "        reward = observation[\"Y\"]\n",
    "        done = (self.counter >= self.episode)\n",
    "        info = \"Round n°{}\".format(self.counter)\n",
    "        self.counter += 1\n",
    "        return (observation, reward, done, info)\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        observation = {}\n",
    "        for var in self.variables:\n",
    "            observation[var] = 0\n",
    "        self.regrets = []\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "We want to explore the ability to learn causal structures and causal effects between variables on decision making frameworks. We start with causal bandits which are the simplest frameworks with explicit causal structures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Generic class for agent model implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: list[variable, float]):\n",
    "        \"\"\"\n",
    "        Instantiate a model for decision making on causal bandit envs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actions : list[variable, float]\n",
    "            list of possible actions. \n",
    "        \n",
    "        \"\"\"\n",
    "        self.actions = actions\n",
    "\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        \"\"\" default is random sample. \n",
    "        \"\"\"\n",
    "        action = random.choices(self.actions)\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = [0.2,0.8,0.2,0.8, 1.0]\n",
    "ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "\n",
    "agent = Agent(ACTIONS)\n",
    "env = BernoulliBandit(PARAMS)\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "hist_observations = []\n",
    "hist_randag_rewards = []\n",
    "hist_randag_actions = []\n",
    "\n",
    "for t in range(40):\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    hist_randag_rewards.append(reward)\n",
    "    hist_randag_actions.append(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"End of this episode.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson Sampling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSAgent(Agent):\n",
    "    \"\"\" Thompson Sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, actions: list[variable, float]):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(actions)\n",
    "        self.beta_params = [(1, 1) for k in range(self.K)]\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "\n",
    "        # update beta params\n",
    "        index = self.actions.index((\"X\", observation[\"X\"]))\n",
    "        alpha, beta = self.beta_params[index]\n",
    "        self.beta_params[index] = (alpha + observation[\"Y\"], beta + 1 - observation[\"Y\"])\n",
    "\n",
    "        # sample model\n",
    "        theta_est = []\n",
    "        for k in range(self.K):\n",
    "            theta_est.append(st.beta.rvs(self.beta_params[k][0], self.beta_params[k][1]))\n",
    "\n",
    "        # select action\n",
    "        action_index = np.argmax(theta_est)\n",
    "        return self.actions[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TSAgent(ACTIONS)\n",
    "env = BernoulliBandit(PARAMS)\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "hist_observations = []\n",
    "hist_tsag_rewards = []\n",
    "hist_tsag_actions = []\n",
    "\n",
    "for t in range(40):\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    hist_tsag_rewards.append(reward)\n",
    "    hist_tsag_actions.append(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"End of this episode.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative reward')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(hist_tsag_rewards))\n",
    "plt.title('Cumulative reward')\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Cumulative reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Causal Thompson Sampling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCTSAgent(Agent):\n",
    "    \"\"\"\n",
    "    OCTS agent without causal graph knowledge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: list[variable, float]):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(self.actions)\n",
    "        self.n_part = 4\n",
    "        self.dirc = np.ones([self.n_part, self.K], dtype=int)\n",
    "        self.beta = np.ones([self.n_part, 2], dtype=int)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        \"\"\" Select action based on OCTS algorithm\n",
    "            Note that X is always manipulated here\n",
    "            so this does not work for an observational setting.\n",
    "        \"\"\"\n",
    "\n",
    "        # update\n",
    "        list_observation = [(key, value) for key, value in observation.items()]\n",
    "        z = sum([2**i * list_observation[i][1] for i in range(len(list_observation)-1)])\n",
    "        self.dirc[z, int(observation[\"X\"])] += 1\n",
    "        self.beta[z, 1 - int(observation[\"Y\"])] += 1\n",
    "\n",
    "        # sample        \n",
    "        success_chance = np.zeros(self.K)\n",
    "\n",
    "        for a in range(self.K):\n",
    "            partition_prob = np.random.dirichlet(self.dirc[:,a]).reshape(-1,1)\n",
    "            sample_prob = np.random.beta(self.beta[:, 0], self.beta[:, 1]).reshape(-1,1)\n",
    "            success_chance[a] = sum(np.matmul(sample_prob.T, partition_prob))[0]\n",
    "            \n",
    "        # select action\n",
    "        action_index = np.argmax(success_chance)\n",
    "        \n",
    "        return self.actions[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCTSAgent_with_graph(Agent):\n",
    "    \"\"\"\n",
    "    OCTS agent without causal graph knowledge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: list[variable, float], env):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(self.actions)\n",
    "        self.n_part = 4\n",
    "        self.beta = np.ones([self.n_part, self.K], dtype=int)\n",
    "        self.env = env\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "\n",
    "        # update\n",
    "        list_observation = [(key, value) for key, value in observation.items()]\n",
    "        z = sum([2**i * list_observation[i][1] for i in range(len(list_observation)-1)])\n",
    "        \n",
    "\n",
    "        # sample        \n",
    "        success_chance = np.zeros(self.K)\n",
    "\n",
    "        for a in range(self.K):\n",
    "            partition_prob = np.array([[st.bernoulli.rvs(self.env.params[4]), st.bernoulli.rvs(1-self.env.params[4])]]).reshape(-1, 1)\n",
    "            sample_prob = np.random.beta(self.beta[:,0], self.beta[:, 1]).reshape(-1,1)\n",
    "            success_chance[a] = sum(np.matmul(sample_prob, partition_prob.T))[0]\n",
    "            print(\"success_chance:{}\".format(success_chance))\n",
    "\n",
    "        # select action\n",
    "        action_index = np.argmax(success_chance)\n",
    "\n",
    "        return self.actions[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OCTSAgent(ACTIONS)\n",
    "PARAMS = [1.0, 0.0, 0.0, 1.0, 1.0]\n",
    "\n",
    "env = BernoulliCausalBandit(PARAMS)\n",
    "\n",
    "observation = env.reset()\n",
    "#observation.pop(\"Z\")\n",
    "\n",
    "hist_observations = []\n",
    "hist_octsag_rewards = []\n",
    "hist_octsag_actions = []\n",
    "\n",
    "for t in range(40):\n",
    "\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    hist_octsag_rewards.append(reward)\n",
    "    hist_octsag_actions.append(action)\n",
    "    if done:\n",
    "        print(\"End of this episode.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x156e23d60>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = []\n",
    "for i in range(len(hist_octsag_actions)):\n",
    "    var, val = hist_octsag_actions[i]\n",
    "    tmp.append(val)\n",
    "plt.plot(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Inference Truncated Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "class AIAgent(Agent):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    def __init__(self, actions: list[variable, float]):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(actions)\n",
    "        self.beta_params = [(1, 1) for k in range(self.K)]\n",
    "        self.precision = 1.0\n",
    "        \n",
    "    def choose_action(self, observation, tmp = []):\n",
    "\n",
    "        # update beta params\n",
    "        index = self.actions.index((\"X\", observation[\"X\"]))\n",
    "        alpha_x, beta_x = self.beta_params[index]\n",
    "        self.beta_params[index] = (alpha_x + observation[\"Y\"], beta_x + 1 - observation[\"Y\"])\n",
    "\n",
    "        # compute approximate free energy\n",
    "        est_eff = []\n",
    "        tmp2 = []\n",
    "        for k in range(self.K):\n",
    "            alpha, beta = self.beta_params[k]\n",
    "            nu = alpha + beta\n",
    "            mu = alpha/nu\n",
    "            est_eff.append(2*self.precision*mu + 1/(2*nu))\n",
    "            tmp2.append(2*self.precision*mu + 1/(2*nu))\n",
    "\n",
    "        tmp.append(scipy.special.softmax(tmp2)) \n",
    "        #print(\"g:{}\".format(est_eff))\n",
    "        # select action\n",
    "        action_index = np.argmax(est_eff)\n",
    "\n",
    "        return self.actions[action_index] #, tmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = [1.0, 0.0, 0.0, 1.0, 0.0]\n",
    "\n",
    "agent = AIAgent(ACTIONS)\n",
    "env = BernoulliCausalBandit(PARAMS)\n",
    "\n",
    "observation =  {\"X\":0, \"Y\":1,\"Z\":0}# env.reset()\n",
    "\n",
    "hist_observations = []\n",
    "hist_aiag_rewards = []\n",
    "hist_aiag_actions = []\n",
    "tmp = []\n",
    "for t in range(40):\n",
    "\n",
    "    action = agent.choose_action(observation)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    hist_aiag_rewards.append(reward)\n",
    "    hist_aiag_actions.append(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"End of this episode.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x157b34a30>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   probabilities \u001b[38;5;241m=\u001b[39m exponential \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exponential)\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m probabilities\n\u001b[0;32m----> 8\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(softmax(\u001b[43mtmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m      9\u001b[0m axs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(softmax(tmp[\u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "def softmax(vec):\n",
    "  exponential = np.exp(vec)\n",
    "  probabilities = exponential / np.sum(exponential)\n",
    "  return probabilities\n",
    "\n",
    "axs[0].plot(softmax(tmp[0]))\n",
    "axs[1].plot(softmax(tmp[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(len(hist_octsag_actions)):\n",
    "    var, val = hist_aiag_actions[i]\n",
    "    tmp.append(val)\n",
    "plt.plot(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Inference Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangle(x):\n",
    "    output = None\n",
    "    if x < 1/2:\n",
    "        output = 2*x\n",
    "    else:\n",
    "        output = -2*x + 2\n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIBanditAgent(Agent):\n",
    "\n",
    "    def __init__(self, actions: list[variable, float], param: float, precision:float = 0.1):\n",
    "        super().__init__(actions)\n",
    "        self.K = len(actions)\n",
    "        self.beta_params = [(1, 1) for k in range(self.K)]  \n",
    "        self.precision = precision\n",
    "        self.rho = triangle(param)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "\n",
    "        # update beta params\n",
    "        index = self.actions.index((\"X\", observation[\"X\"]))\n",
    "        alpha_x, beta_x = self.beta_params[index]\n",
    "        self.beta_params[index] = (alpha_x + observation[\"Y\"], beta_x + 1 - observation[\"Y\"])\n",
    "        \n",
    "        # compute approximate free energy\n",
    "        g = []\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            alpha, beta = self.beta_params[k]\n",
    "            nu_1 = alpha + beta\n",
    "            mu_1 = alpha/nu_1\n",
    "            mu = mu_1 + self.rho*(0.5 - mu_1)\n",
    "\n",
    "            g_a = -2*self.precision*(1-self.rho)*mu_1 + mu*np.log(mu) + (1-mu)*np.log(1 - mu) - \\\n",
    "                 (1 - self.rho)*(mu_1*special.digamma(alpha) + 1 - mu_1*special.digamma(beta)) + \\\n",
    "                    (1 - self.rho)*(special.digamma(nu_1)-1/nu_1) + 1\n",
    "            g.append(g_a)\n",
    "        \n",
    "        # select action\n",
    "        action_index = np.argmax(g)\n",
    "        \n",
    "        return self.actions[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = [1.0, 0.0, 0.0, 1.0, 0.5]\n",
    "agent = AIBanditAgent(ACTIONS, param = PARAMS[4])\n",
    "#agent = AIAgent(ACTIONS)\n",
    "env = BernoulliCausalBandit(PARAMS)\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "hist_observations = []\n",
    "hist_aiag_rewards = []\n",
    "hist_aiag_actions = []\n",
    "\n",
    "for t in range(40):\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    hist_aiag_rewards.append(reward)\n",
    "    hist_aiag_actions.append(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"End of this episode.\")\n",
    "        break\n",
    "\n",
    "plt.plot(np.cumsum(hist_aiag_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(len(hist_octsag_actions)):\n",
    "    var, val = hist_aiag_actions[i]\n",
    "    tmp.append(val)\n",
    "plt.plot(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations and Results\n",
    "### Bernoulli Causal Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_regrets_causal_bandit():\n",
    "    PARAMS = [0.9,0.1,0.6,0.4, 0.5]\n",
    "    ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "\n",
    "    agents = [AIAgent(ACTIONS), TSAgent(ACTIONS), OCTSAgent(ACTIONS), Agent(ACTIONS)]#, AIBanditAgent(ACTIONS, PARAMS[4])]\n",
    "    envs = [BernoulliCausalBandit(PARAMS) for i in range(len(agents))]\n",
    "    #agents = [AIAgent(ACTIONS), TSAgent(ACTIONS), OCTSAgent_with_graph(ACTIONS, envs[3]), Agent(ACTIONS)]\n",
    "    regrets = [[] for i in range(len(agents))]\n",
    "\n",
    "    for i in tqdm(range(len(agents))): \n",
    "        current_agent = agents[i]\n",
    "        current_env = envs[i]\n",
    "        current_regret = regrets[i]\n",
    "\n",
    "        for t in range(1000):\n",
    "            observation = current_env.reset()\n",
    "            for k in range(40):\n",
    "                if (i==0):\n",
    "                    action, _ = current_agent.choose_action(observation)\n",
    "                else:\n",
    "                    action = current_agent.choose_action(observation)\n",
    "                observation, reward, done, info = current_env.step(action)\n",
    "            current_regret.append(current_env.regrets)\n",
    "        \n",
    "        current_cumul_reg = np.cumsum(current_regret, axis=0)\n",
    "        _,m = np.shape(current_cumul_reg)\n",
    "        means = np.asarray([np.mean(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        stds = np.asarray([np.std(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        x = np.arange(m)\n",
    "\n",
    "        plt.plot(x, means)\n",
    "        plt.fill_between(x, means - stds, means + stds, alpha = 0.2, label='_nolegend_')\n",
    "\n",
    "    plt.legend(['Active Inference',  'TS', 'OCTS',  'random', 'AI Bandit'])\n",
    "    plt.title('Cumulative Regrets - Causal Bandit')\n",
    "\n",
    "plot_cumulative_regrets_causal_bandit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli standard bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_regrets_bandit():\n",
    "    PARAMS = [0.6,0.4]\n",
    "    ACTIONS = [(\"X\",0),(\"X\",1)]\n",
    "\n",
    "    agents = [AIAgent(ACTIONS), TSAgent(ACTIONS), OCTSAgent(ACTIONS), Agent(ACTIONS)]#, AIBanditAgent(ACTIONS, 1.0)]\n",
    "    envs = [BernoulliBandit(PARAMS) for i in range(len(agents))]\n",
    "    regrets = [[] for i in range(len(agents))]\n",
    "\n",
    "    for i in tqdm(range(len(agents))): \n",
    "        current_agent = agents[i]\n",
    "        current_env = envs[i]\n",
    "        current_regret = regrets[i]\n",
    "\n",
    "        for t in range(1000):\n",
    "            observation = current_env.reset()\n",
    "            for k in range(40):\n",
    "                action = current_agent.choose_action(observation)\n",
    "                observation, reward, done, info = current_env.step(action)\n",
    "                print(t, np.shape(observation))\n",
    "            current_regret.append(current_env.regrets)\n",
    "        current_cumul_reg = np.cumsum(current_regret, axis=0)\n",
    "        _,m = np.shape(current_cumul_reg)\n",
    "        means = np.asarray([np.mean(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        stds = np.asarray([np.std(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        x = np.arange(m)\n",
    "\n",
    "        plt.plot(x, means)\n",
    "        plt.fill_between(x, means - stds, means + stds, alpha = 0.2, label='_nolegend_')\n",
    "\n",
    "    plt.legend(['Active Inference',  'TS', 'OCTS',  'random', 'AI Bandit'])\n",
    "    plt.title('Cumulative Regrets - Bernoulli Bandit')\n",
    "\n",
    "plot_cumulative_regrets_bandit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Plots (from an external point of view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probs(rewards, actions):\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        # compute probability to win when the agent's play\n",
    "        win_play = [(action[1] and reward) for (action, reward) in zip(actions, rewards)]\n",
    "        num_win_play = [np.sum(win_play[:i]) for i in range(len(win_play))]\n",
    "        num_play = [np.sum([action[1] for action in actions[:i]]) for i in range(len(actions))]\n",
    "        p_win_play = [win_play_t/play_total_t for (win_play_t, play_total_t) in zip(num_win_play, num_play)]\n",
    "\n",
    "        # compute the probability to win when the agent doesn't play\n",
    "        win_no_play = [int(not(action[1]) and reward) for (action, reward) in zip(actions, rewards)]\n",
    "        num_win_no_play = [np.sum(win_no_play[:i]) for i in range(len(win_no_play))]\n",
    "        num_no_play = [np.sum([int(not(action[1])) for action in actions[:i]]) for i in range(len(actions))]\n",
    "        p_win_no_play = [win_no_play_t/no_play_t for (win_no_play_t, no_play_t) in zip(num_win_no_play, num_no_play)]\n",
    "\n",
    "        # compute probability of play\n",
    "        p_win_play = [(0.5 if np.isnan(p) else p) for p in p_win_play]\n",
    "        p_win_no_play = [(0.5 if np.isnan(p) else p) for p in p_win_no_play]\n",
    "\n",
    "        p_play = [num_play/i for (i, num_play) in enumerate(num_play)]\n",
    "\n",
    "        return p_win_play, p_win_no_play, p_play\n",
    "\n",
    "def plot_prob(agent_name: str, switch: float=0.0, reset_agent: bool=True, change: bool = False):\n",
    "\n",
    "    \n",
    "    index = [\"0.6\", \"0.4\", \"0.0\", \"-0.4\"]\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20,5))\n",
    "    fig.suptitle(agent_name)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")   \n",
    "                \n",
    "        for k in range(4):\n",
    "            list_params = [[0.8, 0.2, 0.2, 0.8, switch],\n",
    "                                    [0.7, 0.3, 0.3, 0.7, switch],\n",
    "                                    [0.5, 0.5, 0.5, 0.5, switch],\n",
    "                                    [0.3, 0.7, 0.7, 0.3, switch]]\n",
    "            PARAMS = list_params[k]\n",
    "            env = BernoulliChangingCausalBandit(PARAMS)\n",
    "            observation = {'X': 0, 'Y': 0, 'Z': 0}\n",
    "            \n",
    "\n",
    "            if not(reset_agent):\n",
    "                agent = agent_name(ACTIONS)\n",
    "\n",
    "            list_pwp = []\n",
    "            list_pwnp = []\n",
    "            list_play = []\n",
    "\n",
    "            for i in tqdm(range(1000)):\n",
    "\n",
    "                hist_observations = []\n",
    "                hist_randag_rewards = []\n",
    "                hist_randag_actions = []\n",
    "                observation = env.reset()\n",
    "\n",
    "                if reset_agent:\n",
    "                    agent = agent_name(ACTIONS)\n",
    "\n",
    "                for t in range(40):\n",
    "                    \n",
    "                    if ((t == 20) and change):\n",
    "                        observation = env.reset(switch)\n",
    "\n",
    "\n",
    "                    action = agent.choose_action(observation)\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    hist_randag_rewards.append(reward)\n",
    "                    hist_randag_actions.append(action)\n",
    "\n",
    "                    #if done:\n",
    "                    #     print(\"End of this episode.\")\n",
    "                    #    break\n",
    "                    \n",
    "\n",
    "                randag_pwp, randag_pwnp, p_play = compute_probs(hist_randag_rewards, hist_randag_actions)\n",
    "                list_pwp.append(randag_pwp)\n",
    "                list_pwnp.append(randag_pwnp)\n",
    "                list_play.append(p_play)\n",
    "\n",
    "\n",
    "            list_pwp = np.asarray(list_pwp)\n",
    "            _, m = np.shape(list_pwp)\n",
    "            \n",
    "            pwp_means = np.asarray(([np.nanmean(list_pwp[:,j]) for j in range(m)]))\n",
    "            pwp_stds = np.asarray(([np.nanstd(list_pwp[:,j]) for j in range(m)]))\n",
    "\n",
    "            x = np.arange(m) \n",
    "            \n",
    "            axs[0].plot(x, pwp_means, label=\"P(win|play)={}\".format(PARAMS[0]))\n",
    "            axs[0].fill_between(x, pwp_means-pwp_stds, pwp_means+pwp_stds, alpha = 0.2, )\n",
    "            axs[0].set_title('P(win|play)')\n",
    "            axs[0].legend()\n",
    "\n",
    "            list_pwnp = np.asarray(list_pwnp)\n",
    "            _, m = np.shape(list_pwnp)\n",
    "            \n",
    "            pwnp_means = np.asarray(([np.nanmean(list_pwnp[:,j]) for j in range(m)]))\n",
    "            pwnp_stds = np.asarray(([np.nanstd(list_pwnp[:,j]) for j in range(m)]))\n",
    "\n",
    "            pwnp_means = np.nan_to_num(pwnp_means)\n",
    "            pwnp_stds = np.nan_to_num(pwnp_stds)\n",
    "            \n",
    "            x = np.arange(m)\n",
    "\n",
    "            axs[1].plot(x, pwnp_means, label=\"P(win|not play)={}\".format(PARAMS[1]))\n",
    "            axs[1].fill_between(x, pwnp_means-pwnp_stds, pwnp_means+pwnp_stds, alpha = 0.2)\n",
    "            axs[1].set_title('P(win|not play)')\n",
    "            axs[1].legend()\n",
    "\n",
    "\n",
    "            dp_means = np.nan_to_num(pwp_means - pwnp_means)\n",
    "            dp_stds = np.nan_to_num(pwp_stds - pwnp_stds)\n",
    "\n",
    "            axs[2].plot(x, dp_means,  label=\"dP={}\".format(np.round(PARAMS[0]-PARAMS[1], 4)))\n",
    "            axs[2].fill_between(x, dp_means-dp_stds, dp_means+dp_stds, alpha=0.2)\n",
    "            axs[2].set_title(\"dP\")\n",
    "            axs[2].legend()\n",
    "\n",
    "            list_play = np.asarray(list_play)\n",
    "            _, m = np.shape(list_play)\n",
    "            p_play_mean = np.asarray(([np.nanmean(list_play[:, j]) for j in range(m)]))\n",
    "            p_play_std = np.asarray(([np.nanstd(list_play[:, j]) for j in range(m)]))\n",
    "            mean = np.nanmean(p_play_mean)\n",
    "            std = np.nanmean(p_play_std)\n",
    "            axs[3].bar(index[k], (mean), yerr=std)\n",
    "            axs[3].set_title(\"P(Play)\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randag_pwp, randag_pwnp, _ = compute_probs(hist_randag_rewards, hist_randag_actions)\n",
    "tsag_pwp, tsag_pwnp, _ = compute_probs(hist_tsag_rewards, hist_tsag_actions)\n",
    "octsag_pwp, octsag_pwnp, _ = compute_probs(hist_octsag_rewards, hist_octsag_actions)\n",
    "aiag_pwp, aiag_pwnp, _ = compute_probs(hist_aiag_rewards, hist_aiag_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prob(Agent, switch=0.4, reset_agent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TS Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prob(TSAgent, reset_agent=True, switch=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCTS agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prob(OCTSAgent, reset_agent=True, switch=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active Inference agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prob(AIAgent, reset_agent=True, switch=0.4)\n",
    "#agent = AIBanditAgent(ACTIONS, PARAMS[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p(play) over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_p_play(agent_name, reset_agent=True, switch=0.4):\n",
    "\n",
    "    list_params = [[0.8, 0.2, 0.2, 0.8, switch],\n",
    "                    [0.7, 0.3, 0.3, 0.7, switch],\n",
    "                    [0.5, 0.5, 0.5, 0.5, switch],\n",
    "                    [0.3, 0.7, 0.7, 0.3, switch]]\n",
    "    p_play_mean = []\n",
    "    p_play_std = []\n",
    "    for k in range(4):\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "\n",
    "            warnings.simplefilter(\"ignore\") \n",
    "\n",
    "            PARAMS = list_params[k]\n",
    "            env = BernoulliChangingCausalBandit(PARAMS)\n",
    "            observation = env.reset()  \n",
    "\n",
    "            if not(reset_agent):\n",
    "                agent = Agent(ACTIONS)\n",
    "\n",
    "            list_play = []\n",
    "\n",
    "            for i in tqdm(range(1000)):\n",
    "\n",
    "                hist_observations = []\n",
    "                hist_ag_rewards = []\n",
    "                hist_ag_actions = []\n",
    "\n",
    "                observation = env.reset()\n",
    "\n",
    "                if reset_agent:\n",
    "                    agent = agent_name(ACTIONS)\n",
    "\n",
    "                for t in range(40):\n",
    "                    if (t == 20 ):\n",
    "                        observation = env.reset(switch)\n",
    "                    action = agent.choose_action(observation)\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "\n",
    "                    hist_ag_rewards.append(reward)\n",
    "                    hist_ag_actions.append(action) \n",
    "\n",
    "                    if done:\n",
    "                        print(\"End of this episode.\")\n",
    "                        \n",
    "                        break\n",
    "\n",
    "                _, _, p_play = compute_probs(hist_ag_rewards, hist_ag_actions)\n",
    "                list_play.append(p_play)\n",
    "\n",
    "            list_play = np.asarray(list_play)\n",
    "            _, m = np.shape(list_play)\n",
    "            p_play_mean.append(np.asarray(([np.nanmean(list_play[:, j]) for j in range(m)])))\n",
    "            p_play_std.append(np.asarray(([np.nanstd(list_play[:, j]) for j in range(m)])))\n",
    "\n",
    "    return p_play_mean, p_play_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0, ps0 = compute_p_play(Agent)\n",
    "p1, ps1 = compute_p_play(TSAgent)\n",
    "p2, ps2 = compute_p_play(OCTSAgent)\n",
    "p3, ps3 = compute_p_play(AIAgent)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20,5))\n",
    "m = np.shape(p0)\n",
    "x = np.arange(m[1])\n",
    "\n",
    "for k in range(4):\n",
    "    for j in range(4):\n",
    "        axs[k].plot(x, locals()[\"p\"+str(k)][j], label=\"P(win|not play)={}\".format(PARAMS[1]))\n",
    "        axs[k].fill_between(x, locals()[\"p\"+str(k)][j]-locals()[\"ps\"+str(k)][j], locals()[\"p\"+str(k)][j]+locals()[\"ps\"+str(k)][j], alpha = 0.2)\n",
    "        axs[k].set_title('P(play)')\n",
    "        axs[k].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(softmax(p3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Grid Search\n",
    "\n",
    "On s'intéresse à la performance relative de OCTS et TS. Pour cela, on va parcourir l'espace des paramètres qui nous intéresse, en l'occurence, $z \\sim P(z) = Bern(\\theta)$, le paramètre theta dans la plage $[ 0, 1 ]$ et on va regarder la différence en moyenne entre les deux algorithmes. Pour $\\theta$ on va aller de 0.01. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def compute_OCTS_TS_performance_causal_bandit(PARAMS):\n",
    "    envs = [BernoulliBandit(PARAMS) for i in range(2)]\n",
    "    agents = [TSAgent(ACTIONS), OCTSAgent(ACTIONS)]\n",
    "    regrets = [[], []]\n",
    "    means = [[],[]]\n",
    "    stds = [[], []]\n",
    "\n",
    "    for i in range(len(agents)): \n",
    "        current_agent = agents[i]\n",
    "        current_env = envs[i]\n",
    "        current_regret = regrets[i]\n",
    "        current_mean = means[i]\n",
    "        current_std = stds[i]\n",
    "\n",
    "        for t in (range(1000)):\n",
    "            observation = current_env.reset()\n",
    "            for k in range(40):\n",
    "                action = current_agent.choose_action(observation)\n",
    "                observation, reward, done, info = current_env.step(action)\n",
    "            current_regret.append(current_env.regrets)\n",
    "\n",
    "        current_cumul_reg = np.cumsum(current_regret, axis=0)\n",
    "        _,m = np.shape(current_cumul_reg)\n",
    "        mean = np.asarray([np.mean(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        std = np.asarray([np.std(current_cumul_reg[j, :]) for j in range(m)])\n",
    "        current_mean.append(mean)\n",
    "        current_std.append(std)\n",
    "\n",
    "    dif_means = np.mean(np.array(means[1]) - np.array(means[0]))\n",
    "    dif_stds = (np.array(stds[1]) - np.array(stds[0]))\n",
    "\n",
    "    return dif_means, dif_stds\n",
    "\n",
    "theta_range = np.arange(0, 1, 0.1)\n",
    "mean_points = []\n",
    "std_points = []\n",
    "mean_total = []\n",
    "std_total = []\n",
    "\n",
    "for k in tqdm(range(10)):\n",
    "    for i in (range(len(theta_range))):\n",
    "        PARAMS = [0.3, 0.7, 0.7, 0.3, theta_range[i]]\n",
    "        dif_means, dif_stds = compute_OCTS_TS_performance_causal_bandit(PARAMS)\n",
    "        mean_points.append(dif_means)\n",
    "        std_points.append(dif_stds)\n",
    "    mean_total.append(mean_points)\n",
    "    std_total.append(std_points)\n",
    "\n",
    "n, m = np.shape(mean_total)\n",
    "mean_total = np.array(mean_total)\n",
    "std_total = np.array(std_total)\n",
    "\n",
    "mean = np.asarray([np.mean(mean_total[ :, j]) for j in range(n)])\n",
    "std = np.asarray([np.std(std_total[:, j ]) for j in range(n)])\n",
    "plt.plot(theta_range, mean)\n",
    "plt.fill_between(theta_range, mean - std, mean + std, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Effects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
