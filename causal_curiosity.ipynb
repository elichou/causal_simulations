{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CausalBandit:\n",
    "    def __init__(self, true_parameters):\n",
    "        self.true_parameters = true_parameters\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.true_parameters[arm])\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.total_rewards = [0] * num_arms\n",
    "        self.total_pulls = [0] * num_arms\n",
    "\n",
    "    def choose_arm(self):\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.total_rewards[arm] += reward\n",
    "        self.total_pulls[arm] += 1\n",
    "\n",
    "class EpsilonGreedyAgent(Agent):\n",
    "    def __init__(self, num_arms, epsilon):\n",
    "        super().__init__(num_arms)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_arm(self):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.choice(self.num_arms)\n",
    "        else:\n",
    "            return np.argmax([r / max(1, p) for r, p in zip(self.total_rewards, self.total_pulls)])\n",
    "\n",
    "class UCB1Agent(Agent):\n",
    "    def choose_arm(self):\n",
    "        ucb_values = [r / max(1, p) + np.sqrt(2 * np.log(sum(self.total_pulls)) / max(1, p)) for r, p in zip(self.total_rewards, self.total_pulls)]\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "class ThompsonSamplingAgent(Agent):\n",
    "    def choose_arm(self):\n",
    "        sampled_parameters = [np.random.beta(r + 1, p - r + 1) for r, p in zip(self.total_rewards, self.total_pulls)]\n",
    "        return np.argmax(sampled_parameters)\n",
    "\n",
    "def grid_search(agents, true_parameters_grid, num_trials):\n",
    "    regrets = {agent.__class__.__name__: [] for agent in agents}\n",
    "    for true_parameters in true_parameters_grid:\n",
    "        for agent in agents:\n",
    "            bandit = CausalBandit(true_parameters)\n",
    "            for _ in range(num_trials):\n",
    "                chosen_arm = agent.choose_arm()\n",
    "                reward = bandit.pull_arm(chosen_arm)\n",
    "                agent.update(chosen_arm, reward)\n",
    "                regret = max(true_parameters) - true_parameters[chosen_arm]\n",
    "                regrets[agent.__class__.__name__].append(regret)\n",
    "    return regrets\n",
    "\n",
    "# Define parameters\n",
    "num_arms = 4\n",
    "true_parameters_grid = [[0.1, 0.2, 0.3, 0.4], [0.2, 0.3, 0.4, 0.5]]  # Example grid of Bernoulli parameters\n",
    "num_trials = 1000\n",
    "epsilon_values = [0.1, 0.2, 0.3]  # Example epsilon values for epsilon-greedy agent\n",
    "\n",
    "# Initialize agents\n",
    "agents = [EpsilonGreedyAgent(num_arms, epsilon) for epsilon in epsilon_values] + [UCB1Agent(num_arms), ThompsonSamplingAgent(num_arms)]\n",
    "\n",
    "# Perform grid search\n",
    "regrets = grid_search(agents, true_parameters_grid, num_trials)\n",
    "\n",
    "# Calculate cumulative regrets\n",
    "cumulative_regrets = {agent: sum(regrets[agent]) for agent in regrets}\n",
    "\n",
    "print(\"Cumulative Regrets:\")\n",
    "for agent, regret in cumulative_regrets.items():\n",
    "    print(f\"{agent}: {regret}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the environment\n",
    "class Environment:\n",
    "    def __init__(self, num_vars, causal_graph):\n",
    "        self.num_vars = num_vars\n",
    "        self.causal_graph = causal_graph\n",
    "        self.state = np.random.normal(0, 0.1, num_vars)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply the action (intervention) to the corresponding variable\n",
    "        self.state[action] = np.random.normal()\n",
    "\n",
    "        # Update the state based on the causal graph\n",
    "        for node in nx.topological_sort(self.causal_graph):\n",
    "            parents = list(self.causal_graph.predecessors(node))\n",
    "            if parents:\n",
    "                self.state[node] = sum(self.state[parent] for parent in parents) + np.random.normal(0, 0.1)\n",
    "\n",
    "        # Compute the reward (dummy implementation)\n",
    "        reward = np.abs(np.sum(self.state))\n",
    "\n",
    "        return self.state.copy(), reward\n",
    "\n",
    "# Define the agent\n",
    "class CausalAgent:\n",
    "    def __init__(self, num_vars, causal_graph):\n",
    "        self.num_vars = num_vars\n",
    "        self.causal_graph = causal_graph\n",
    "        self.causal_model = nx.DiGraph()\n",
    "        self.intervention_count = np.zeros((num_vars, num_vars))\n",
    "        self.causal_model_history = []  # Store causal model updates\n",
    "        self.max_history_size = 10  # Maximum number of stored causal models\n",
    "\n",
    "    def store_causal_model(self):\n",
    "      # Append the current causal model to the history list\n",
    "        self.causal_model_history.append(self.causal_model.copy())\n",
    "        \n",
    "        # Trim the history list if it exceeds the maximum size\n",
    "        if len(self.causal_model_history) > self.max_history_size:\n",
    "            self.causal_model_history = self.causal_model_history[-self.max_history_size:]\n",
    "\n",
    "\n",
    "    def plot_causal_model_updates(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, causal_model in enumerate(self.causal_model_history):\n",
    "            plt.subplot(len(self.causal_model_history), 1, i+1)\n",
    "            nx.draw(causal_model, with_labels=True)\n",
    "            plt.title(f'Episode {i+1} - Causal Model')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def update_causal_model(self, state, action, next_state):\n",
    "        \n",
    "        # Simplified causal discovery rules\n",
    "        self.causal_model.add_nodes_from(range(self.num_vars))\n",
    "\n",
    "        # Find the variable that changed between the current state and next state\n",
    "        changed_var = np.where(next_state != state)[0]\n",
    "\n",
    "        if len(changed_var) > 0:\n",
    "            changed_var = changed_var[0]  # Select the first changed variable\n",
    "            # Association rule\n",
    "            self.causal_model.add_edge(action, changed_var)\n",
    "\n",
    "            # Causation rule\n",
    "            if not self.causal_model.has_edge(changed_var, action):\n",
    "                self.causal_model.add_edge(changed_var, action)\n",
    "\n",
    "        self.intervention_count[action] += 1\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        epsilon = 0.1\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.num_vars - 1)\n",
    "\n",
    "        # Add missing nodes to the causal graph\n",
    "        for i in range(self.num_vars):\n",
    "            if i not in self.causal_model.nodes:\n",
    "                self.causal_model.add_node(i)\n",
    "\n",
    "        # Restrict action set to actionable variables that are ancestors of the reward\n",
    "        actionable_vars = [var for var in range(self.num_vars) if nx.has_path(self.causal_model, var, self.num_vars - 1)]\n",
    "\n",
    "        # Select the action based on intrinsic reward\n",
    "        intrinsic_rewards = [self.compute_intrinsic_reward(var) for var in actionable_vars]\n",
    "        \n",
    "        return actionable_vars[np.argmax(intrinsic_rewards)]\n",
    "\n",
    "    def compute_number_undetermined_relation_in_history(self, var, neighbor):\n",
    "\n",
    "        return\n",
    "    \n",
    "    def compute_number_directed_relation_in_history(self, var, neighbor):\n",
    "        return\n",
    "\n",
    "    def compute_intrinsic_reward(self, var):\n",
    "        # Compute intrinsic reward based on the causal model's stability\n",
    "        neighbors = list(self.causal_model.neighbors(var))\n",
    "        intrinsic_reward = 0\n",
    "        for neighbor in neighbors:\n",
    "            rel_count = self.intervention_count[var, neighbor]\n",
    "            intrinsic_reward += np.arctan(rel_count) / (rel_count + 1e-6)\n",
    "        return intrinsic_reward \n",
    "\n",
    "# Simulate the agent\n",
    "num_vars = 5\n",
    "causal_graph = nx.DiGraph()\n",
    "causal_graph.add_nodes_from(range(num_vars))\n",
    "\n",
    "# Define the causal graph (dummy example)\n",
    "for i in range(num_vars - 1):\n",
    "    causal_graph.add_edge(i, i + 1)\n",
    "\n",
    "env = Environment(num_vars, causal_graph)\n",
    "agent = CausalAgent(num_vars, causal_graph)\n",
    "\n",
    "for episode in range(500):\n",
    "    state = env.state.copy()\n",
    "    for t in range(100):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        agent.update_causal_model(state, action, next_state)\n",
    "        agent.store_causal_model()\n",
    "        state = next_state\n",
    "\n",
    "# Print the learned causal model\n",
    "print(\"Learned Causal Model:\")\n",
    "print(agent.causal_model.edges())\n",
    "print(agent.causal_model_history)\n",
    "agent.plot_causal_model_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to plot the causal model\n",
    "#def plot_causal_model(causal_model, label):\n",
    "#    plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "#    pos = nx.spring_layout(causal_model)  # Layout algorithm for node positioning\n",
    "#\n",
    "#    # Draw the causal model with adjusted parameters\n",
    "#    nx.draw(causal_model, pos, with_labels=True, node_size=1000, font_size=10, node_color='skyblue', edge_color='gray', width=1.5, alpha=0.8)\n",
    "#    plt.title('Causal Model')\n",
    "#    plt.legend([label], loc='upper left')  # Add the episode label to the legend\n",
    "#    plt.show()\n",
    "#\n",
    "## Plot the stored causal models\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#for i, causal_model in enumerate(agent.causal_model_history):\n",
    "#    plot_causal_model(causal_model, label=f'Episode {i+1}')\n",
    "#plt.xlabel('Nodes')\n",
    "#plt.ylabel('Edges')\n",
    "#plt.title('Evolution of Causal Model over Episodes')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store episode rewards\n",
    "episode_rewards = []\n",
    "\n",
    "# Simulate the agent and collect rewards\n",
    "for episode in range(500):\n",
    "    state = env.state.copy()\n",
    "    total_reward = 0\n",
    "    for t in range(100):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        agent.update_causal_model(state, action, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "# Plot the cumulative reward over episodes\n",
    "plt.plot(range(1, len(episode_rewards) + 1), episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward over Episodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#\n",
    "#N = 100\n",
    "## Initialize a dictionary to store action frequencies\n",
    "#action_freq = {action: [0] * N for action in range(num_vars)}\n",
    "#\n",
    "## Simulate the agent and collect action frequencies\n",
    "#for episode in range(500):\n",
    "#    state = env.state.copy()\n",
    "#    reward = 0\n",
    "#    for t in range(N):\n",
    "#        action = agent.select_action(state)\n",
    "#        next_state, _ = env.step(action)\n",
    "#        agent.update_causal_model(state, action, next_state)\n",
    "#        state = next_state\n",
    "#        action_freq[action] += 1  # Increment action frequency\n",
    "#\n",
    "## Calculate action selection frequencies over time and their standard deviations\n",
    "#action_freq_over_time = {action: [sum(action_freq[action][:t]) / (episode + 1) for t in range(1, N + 1)] for action in range(num_vars)}\n",
    "#action_std_over_time = {action: [np.std(action_freq[action][:t]) for t in range(1, N + 1)] for action in range(num_vars)}\n",
    "#\n",
    "#print(action_freq)\n",
    "## Plot action selection frequencies over time with error bars\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#for action in range(num_vars):\n",
    "#    plt.errorbar(range(1, N + 1), action_freq_over_time[action], label=f'Action {action}')\n",
    "#plt.xlabel('Time step')\n",
    "#plt.ylabel('Action selection frequency')\n",
    "#plt.title('Action Selection Frequency over Time with Error Bars')\n",
    "##plt.legend()\n",
    "#plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate action selection frequencies over time\n",
    "#action_freq_over_time = {action: [sum(action_freq[action][:t]) for t in range(1, len(action_freq[action]) + 1)] for action in range(num_vars)}\n",
    "#\n",
    "## Calculate entropy of the action selection distribution over time\n",
    "#entropy_over_time = []\n",
    "#for t in range(len(action_freq_over_time[0])):\n",
    "#    if t == 0:\n",
    "#        entropy_over_time.append(0)  # Skip calculation for the first time step\n",
    "#    else:\n",
    "#        action_probs = [sum(action_freq[action][:t]) / t for action in range(num_vars)]\n",
    "#        entropy = -np.sum([p * np.log(p) for p in action_probs if p > 0])\n",
    "#        entropy_over_time.append(entropy)\n",
    "#\n",
    "## Plot entropy of action selection distribution over time\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.plot(range(1, len(entropy_over_time) + 1), entropy_over_time, label='Entropy')\n",
    "#plt.xlabel('Time step')\n",
    "#plt.ylabel('Entropy')\n",
    "#plt.title('Entropy of Action Selection Distribution over Time')\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rel_counts = np.arange(1, 101)\n",
    "intrinsic_rewards = np.arctan(rel_counts) / (rel_counts + 1e-6)\n",
    "\n",
    "plt.plot(rel_counts, intrinsic_rewards)\n",
    "plt.xlabel('Relative Count')\n",
    "plt.ylabel('Intrinsic Reward')\n",
    "plt.title('Plot of Intrinsic Reward vs Relative Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allons un peu plus loin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Define the number of variables and time steps\n",
    "num_vars = 5\n",
    "T = 10\n",
    "\n",
    "# Generate a random causal graph\n",
    "G = nx.DiGraph()\n",
    "for i in range(num_vars):\n",
    "    G.add_node(i)\n",
    "for i in range(num_vars):\n",
    "    for j in range(i+1, num_vars):\n",
    "        if np.random.rand() < 0.3:\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Define the structural causal model\n",
    "U = np.random.randn(num_vars)\n",
    "F = {i: lambda parents: sum(U[j] for j in parents) + np.random.randn() for i in range(num_vars)}\n",
    "P = {i: [j for j in G.predecessors(i)] for i in range(num_vars)}\n",
    "\n",
    "# Define the reward condition\n",
    "reward_vars = [0, 2, 4]  # Subset of variables to consider for reward\n",
    "reward_range = (-0.5, 0.5)  # Arbitrary range for reward\n",
    "\n",
    "# Define the agent's action space\n",
    "A = [set() for _ in range(num_vars)]\n",
    "for i in range(num_vars):\n",
    "    for j in range(num_vars):\n",
    "        if np.random.rand() < 0.2:\n",
    "            A[i].add(j)\n",
    "\n",
    "# Simulate the reinforcement learning environment\n",
    "def simulate_episode():\n",
    "    state = [F[i](P[i]) for i in range(num_vars)]\n",
    "    cumulative_reward = 0\n",
    "    for t in range(T):\n",
    "        action = np.random.choice(len(A))\n",
    "        manipulated_vars = A[action]\n",
    "        for var in manipulated_vars:\n",
    "            state[var] = np.random.randn()\n",
    "        if all(reward_range[0] <= state[var] <= reward_range[1] for var in reward_vars):\n",
    "            cumulative_reward += 1\n",
    "    return cumulative_reward\n",
    "\n",
    "# Run multiple episodes and print the average cumulative reward\n",
    "num_episodes = 1000\n",
    "total_reward = 0\n",
    "for _ in range(num_episodes):\n",
    "    total_reward += simulate_episode()\n",
    "print(f\"Average cumulative reward: {total_reward / num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the number of variables and time steps\n",
    "num_vars = 5\n",
    "T = 10\n",
    "\n",
    "# Generate a random causal graph\n",
    "G = nx.DiGraph()\n",
    "for i in range(num_vars):\n",
    "    G.add_node(i)\n",
    "for i in range(num_vars):\n",
    "    for j in range(i+1, num_vars):\n",
    "        if np.random.rand() < 0.3:\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Define the structural causal model\n",
    "U = np.random.randn(num_vars)\n",
    "F = {i: lambda parents: sum(U[j] for j in parents) + np.random.randn() for i in range(num_vars)}\n",
    "P = {i: [j for j in G.predecessors(i)] for i in range(num_vars)}\n",
    "\n",
    "# Define the reward condition\n",
    "reward_vars = [0, 2, 4]  # Subset of variables to consider for reward\n",
    "reward_range = (-0.5, 0.5)  # Arbitrary range for reward\n",
    "\n",
    "# Define the agent's action space\n",
    "A = [set() for _ in range(num_vars)]\n",
    "for i in range(num_vars):\n",
    "    for j in range(num_vars):\n",
    "        if np.random.rand() < 0.2:\n",
    "            A[i].add(j)\n",
    "\n",
    "# Q-learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Q-table\n",
    "Q = defaultdict(lambda: np.zeros(len(A)))\n",
    "\n",
    "# Simulate the reinforcement learning environment\n",
    "def simulate_episode(epsilon):\n",
    "    state = [F[i](P[i]) for i in range(num_vars)]\n",
    "    cumulative_reward = 0\n",
    "    for t in range(T):\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(len(A))  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[tuple(state)])  # Exploit\n",
    "        next_state = state.copy()\n",
    "        manipulated_vars = A[action]\n",
    "        for var in manipulated_vars:\n",
    "            next_state[var] = np.random.randn()\n",
    "        reward = 1 if all(reward_range[0] <= next_state[var] <= reward_range[1] for var in reward_vars) else 0\n",
    "        Q[tuple(state)][action] += alpha * (reward + gamma * np.max(Q[tuple(next_state)]) - Q[tuple(state)][action])\n",
    "        state = next_state\n",
    "        cumulative_reward += reward\n",
    "    return cumulative_reward\n",
    "\n",
    "# Run multiple episodes and train the agent\n",
    "num_episodes = 10000\n",
    "total_reward = 0\n",
    "for episode in range(num_episodes):\n",
    "    total_reward += simulate_episode(epsilon)\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode}: Average cumulative reward: {total_reward / (episode + 1)}\")\n",
    "\n",
    "# Test the trained agent\n",
    "num_test_episodes = 100\n",
    "test_reward = 0\n",
    "for _ in range(num_test_episodes):\n",
    "    test_reward += simulate_episode(0)  # Set epsilon to 0 for exploitation\n",
    "print(f\"Average test cumulative reward: {test_reward / num_test_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from cdt import data\n",
    "from cdt.causality.pairwise import ANM\n",
    "from cdt.causality.graph import LiNGAM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the number of variables and time steps\n",
    "num_vars = 5\n",
    "T = 10\n",
    "\n",
    "# Generate a random causal graph\n",
    "G = nx.DiGraph()\n",
    "for i in range(num_vars):\n",
    "    G.add_node(i)\n",
    "for i in range(num_vars):\n",
    "    for j in range(i+1, num_vars):\n",
    "        if np.random.rand() < 0.3:\n",
    "            G.add_edge(i, j)\n",
    "# Define the structural causal model\n",
    "U = np.random.randn(num_vars)\n",
    "F = {i: lambda parents: sum(U[j] for j in parents) + np.random.randn() for i in range(num_vars)}\n",
    "P = {i: [j for j in G.predecessors(i)] for i in range(num_vars)}\n",
    "\n",
    "# Define the reward condition\n",
    "reward_vars = [0, 2, 4]  # Subset of variables to consider for reward\n",
    "reward_range = (-0.5, 0.5)  # Arbitrary range for reward\n",
    "\n",
    "# Define the agent's action space\n",
    "A = [set() for _ in range(num_vars)]\n",
    "for i in range(num_vars):\n",
    "    for j in range(num_vars):\n",
    "        if np.random.rand() < 0.2:\n",
    "            A[i].add(j)\n",
    "\n",
    "# Q-learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Q-table\n",
    "Q = defaultdict(lambda: np.zeros(len(A)))\n",
    "\n",
    "# Causal discovery\n",
    "causal_data = []\n",
    "lingam = None\n",
    "\n",
    "# Simulate the reinforcement learning environment\n",
    "def simulate_episode(epsilon):\n",
    "    global lingam\n",
    "    global causal_data\n",
    "\n",
    "    state = [F[i](P[i]) for i in range(num_vars)]\n",
    "    cumulative_reward = 0\n",
    "    episode_data = []\n",
    "    for t in range(T):\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(len(A))  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[tuple(state)])  # Exploit\n",
    "        next_state = state.copy()\n",
    "        manipulated_vars = A[action]\n",
    "        for var in manipulated_vars:\n",
    "            next_state[var] = np.random.randn()\n",
    "        reward = 1 if all(reward_range[0] <= next_state[var] <= reward_range[1] for var in reward_vars) else 0\n",
    "        Q[tuple(state)][action] += alpha * (reward + gamma * np.max(Q[tuple(next_state)]) - Q[tuple(state)][action])\n",
    "        episode_data.append(dict(zip([f\"X{i}\" for i in range(num_vars)], state + next_state)))  # Store causal data\n",
    "        state = next_state\n",
    "        cumulative_reward += reward\n",
    "\n",
    "    causal_data.append(pd.DataFrame(episode_data))  # Store episode data as DataFrame\n",
    "\n",
    "    # Perform causal discovery regularly\n",
    "    if len(causal_data) >= 100:\n",
    "        causal_df = pd.concat(causal_data, ignore_index=True)\n",
    "        print(causal_df)\n",
    "        #anm = ANM(data.CausalData(causal_df))\n",
    "        lingam = LiNGAM(causal_df)\n",
    "        learned_graph = lingam.estimate()\n",
    "        print(\"Learned graph:\")\n",
    "        print(nx.to_agraph(learned_graph))\n",
    "        causal_data = []  # Reset causal data\n",
    "\n",
    "    return cumulative_reward\n",
    "# Run multiple episodes and train the agent\n",
    "num_episodes = 1000\n",
    "total_reward = 0\n",
    "for episode in (range(num_episodes)):\n",
    "    total_reward += simulate_episode(epsilon)\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}: Average cumulative reward: {total_reward / (episode + 1)}\")\n",
    "\n",
    "# Compare the learned graph with the true graph\n",
    "print(\"True graph:\")\n",
    "print(nx.to_agraph(G))\n",
    "if lingam:\n",
    "    print(\"Final learned graph:\")\n",
    "    print(nx.to_agraph(lingam.estimate()))\n",
    "\n",
    "# Test the trained agent\n",
    "num_test_episodes = 100\n",
    "test_reward = 0\n",
    "for _ in tqdm(range(num_test_episodes)):\n",
    "    test_reward += simulate_episode(0.1)  # Set epsilon to 0 for exploitation\n",
    "print(f\"Average test cumulative reward: {test_reward / num_test_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdt\n",
    "print(cdt.SETTINGS.rpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cdt.SETTINGS.rpath = \"/Applications/R.app/Contents/MacOS/R\"\n",
    "cdt.SETTINGS.rpath = \"/usr/local/bin/R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Callable, Union, Tuple, Dict, List, Optional\n",
    "class StructuralCausalModel: \n",
    "    \"\"\"\n",
    "    Structural Causal Model for Causal Bandit implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        variables: List[str], \n",
    "        structural_equations: Dict[str, Tuple[Callable[[Tuple[int, Union[int, None]]], int], Dict[str, Union[int, None]]]]\n",
    "             \n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate StructuralCausalModel class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        variables: list[variable:str]\n",
    "            List containing the name of the variables. \n",
    "\n",
    "        structural_equations: dict[variable: (func, list[variable:str])]\n",
    "            A dictionary containing the structural relations between variables\n",
    "            and their values. \n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        self.variables = variables # list of variables\n",
    "        self.values = { var: None for var in variables } # list of values taken by each variable\n",
    "        self.structural_equations = structural_equations # functions for each variable\n",
    "        self.causal_graph = self.build_causal_graph(variables, structural_equations) # a causal graph of the SCM\n",
    "\n",
    "    def build_causal_graph(\n",
    "        self, \n",
    "        variables: List[str],\n",
    "        structural_equations: Dict[str, Tuple[Callable[[Tuple[int, Union[int, None]]], int], Dict[str, Union[int, None]]]]\n",
    "        ) -> nx.DiGraph: \n",
    "\n",
    "        \"\"\"\n",
    "        Build a causal graph from variables list and structural equations. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        variables: list[variable]\n",
    "        structural_equations: dict[variable, eqution]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a DiGraph\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        output_graph = nx.DiGraph()\n",
    "        nodes = variables\n",
    "        edges = []\n",
    "        \n",
    "        for variable, equation in structural_equations.items(): \n",
    "            children_node = variable\n",
    "            (function, vars) = equation\n",
    "            \n",
    "            for var, value in vars.items():\n",
    "                parent_node = var\n",
    "                edges.append((parent_node, children_node))\n",
    "\n",
    "        output_graph.add_nodes_from(nodes)\n",
    "        output_graph.add_edges_from(edges)\n",
    "\n",
    "        return output_graph\n",
    "\n",
    "    def graph(self):\n",
    "        \"\"\" Draw the internal causal graph\n",
    "        \"\"\"\n",
    "        nx.draw(self.causal_graph)\n",
    "\n",
    "    def get_sample(self, set_values: Optional[Union[Dict[str, int],None]] = None) -> dict[str, Union[int, None]]:\n",
    "        \"\"\"\n",
    "        Sample from SCM (could be manipulated through set_values).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        set_values: dict[variable, int]\n",
    "            The values fixed by intervention on variables.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        output_assignements : dict[variable, int] \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        output_assignments = {var : None for var in self.variables}\n",
    "\n",
    "        if set_values is not None: \n",
    "            \n",
    "            # Assign values to manipulated variables\n",
    "            for variable, value in set_values.items():\n",
    "                output_assignments[variable] = value\n",
    "                self.values[variable] = value\n",
    "            \n",
    "            # Assign values inside the structural_equations for variables which parents are manipulated variables\n",
    "            for node in self.causal_graph.nodes:\n",
    "                \n",
    "                structural_function, parents = self.structural_equations[node]\n",
    "                for parent in parents.keys():\n",
    "                    if parent in set_values.keys():\n",
    "                        parents[parent] = set_values[parent]\n",
    "\n",
    "            # Assign values to remaining variables for output\n",
    "            for node in nx.topological_sort(self.causal_graph):\n",
    "                if node in set_values.keys():\n",
    "                    pass\n",
    "                else:\n",
    "                    structural_function, parents = self.structural_equations[node]\n",
    "                    output_assignments[node] = structural_function(parents)  \n",
    "                    # when a value is assigned, make sure that it is also assigned in the structural_equations \n",
    "                    for node2 in self.causal_graph.nodes:\n",
    "                        structural_function, parents = self.structural_equations[node2]\n",
    "                        if node in parents.keys():\n",
    "                            parents[node] = output_assignments[node]\n",
    "\n",
    "        else:\n",
    "            for node in nx.topological_sort(self.causal_graph):\n",
    "                    structural_function, parents = self.structural_equations[node]\n",
    "                    output_assignments[node] = structural_function(parents)  \n",
    "                    # when a value is assigned, make sure that it is also assigned in the structural_equations \n",
    "                    for node2 in self.causal_graph.nodes:\n",
    "                        structural_function, parents = self.structural_equations[node2]\n",
    "                        if node in parents.keys():\n",
    "                            parents[node] = output_assignments[node]\n",
    "\n",
    "\n",
    "        return output_assignments  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def directed_edit_distance(G1, G2):\n",
    "    # Calculer les différences entre G1 et G2\n",
    "    G1_edges = set(G1.edges())\n",
    "    G2_edges = set(G2.edges())\n",
    "    \n",
    "    # Trouver les arêtes uniquement présentes dans G1 ou G2\n",
    "    edges_only_in_G1 = G1_edges - G2_edges\n",
    "    edges_only_in_G2 = G2_edges - G1_edges\n",
    "    \n",
    "    # Le coût d'édition peut être simplifié en tant que nombre d'arêtes à ajouter ou à supprimer\n",
    "    edit_cost = len(edges_only_in_G1) + len(edges_only_in_G2)\n",
    "    \n",
    "    return edit_cost\n",
    "\n",
    "# Créer deux graphes orientés pour l'exemple\n",
    "G1 = nx.DiGraph()\n",
    "G2 = nx.DiGraph()\n",
    "\n",
    "# Ajouter des arêtes aux graphes\n",
    "G1.add_edges_from([(1, 2), (2, 3), (3, 4)])\n",
    "G2.add_edges_from([(1, 2), (2, 4), (4, 3)])\n",
    "\n",
    "# Calculer la distance d'édition\n",
    "edit_distance = directed_edit_distance(G1, G2)\n",
    "print(f\"La distance d'édition dirigée est: {edit_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.flow import maximum_flow\n",
    "\n",
    "def compare_graph_flows(G1, G2, source, sink):\n",
    "    # Calculer le flot maximal entre source et sink dans G1\n",
    "    flow_value_G1, _ = maximum_flow(G1, source, sink)\n",
    "    \n",
    "    # Calculer le flot maximal entre source et sink dans G2\n",
    "    flow_value_G2, _ = maximum_flow(G2, source, sink)\n",
    "    \n",
    "    # Comparer les flots maximaux\n",
    "    flow_difference = abs(flow_value_G1 - flow_value_G2)\n",
    "    \n",
    "    return flow_difference\n",
    "\n",
    "# Exemple de création de deux graphes orientés\n",
    "G1 = nx.DiGraph()\n",
    "G2 = nx.DiGraph()\n",
    "\n",
    "# Ajouter des arêtes et des capacités pour G1\n",
    "G1.add_edge('a', 'b', capacity=10)\n",
    "G1.add_edge('b', 'c', capacity=5)\n",
    "\n",
    "# Ajouter des arêtes et des capacités pour G2\n",
    "G2.add_edge('a', 'b', capacity=7)\n",
    "G2.add_edge('b', 'c', capacity=8)\n",
    "\n",
    "# Comparer les flots maximaux entre 'a' (source) et 'c' (puits)\n",
    "difference = compare_graph_flows(G1, G2, 'a', 'c')\n",
    "print(f\"La différence de flot maximal entre G1 et G2 est: {difference}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy_policy(Q, D, state, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy policy to select an action based on Q-values and causal curiosity bonus.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(num_actions)\n",
    "    else:\n",
    "        values = Q[state] + D\n",
    "        return np.argmax(values)\n",
    "\n",
    "def update_causal_graph(observation, causal_discovery_algorithm, graph_distance_metric):\n",
    "    \"\"\"\n",
    "    Update the causal graph based on the new observation and the causal discovery algorithm.\n",
    "    Compute the graph distance between the new and old causal graphs using the specified metric.\n",
    "    \"\"\"\n",
    "    state, action, next_state = observation\n",
    "    new_causal_graph = causal_discovery_algorithm(state, action, next_state)\n",
    "    graph_distance = graph_distance_metric(causal_graph, new_causal_graph)\n",
    "    return new_causal_graph, graph_distance\n",
    "\n",
    "def q_learning_causal_curiosity(env, num_episodes, alpha, epsilon, gamma, beta, delta):\n",
    "    \"\"\"\n",
    "    Q-learning Causal Curiosity Algorithm\n",
    "    \"\"\"\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize Q-values, causal curiosity bonuses, and causal graph\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    D = np.zeros(num_actions)\n",
    "    causal_graph = initialize_causal_graph()\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action using epsilon-greedy policy with Q-values and causal curiosity bonus\n",
    "            action = epsilon_greedy_policy(Q, D, state, epsilon, num_actions)\n",
    "            \n",
    "            # Take action, observe next state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Update Q-values\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "            \n",
    "            # Update causal graph and compute graph distance\n",
    "            new_causal_graph, graph_distance = update_causal_graph((state, action, next_state),\n",
    "                                                                    causal_discovery_algorithm,\n",
    "                                                                    graph_distance_metric)\n",
    "            \n",
    "            # Update causal curiosity bonus\n",
    "            D[action] += delta * D[action] + beta * graph_distance\n",
    "            \n",
    "            # Update state and causal graph\n",
    "            state = next_state\n",
    "            causal_graph = new_causal_graph\n",
    "    \n",
    "    return Q, D, causal_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def sample_from_normal(args):\n",
    "    return int(np.random.normal())\n",
    "\n",
    "class CausalEnvironment:\n",
    "    def __init__(self, num_vars, num_actions, T, density=0.3, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Generate the Structural Causal Model (SCM)\n",
    "        self.variables = [f'X{i}' for i in range(num_vars)]\n",
    "        self.exogenous_vars = [f'U{i}' for i in range(num_vars)]\n",
    "        self.structural_equations = {}\n",
    "\n",
    "         # Generate a directed acyclic graph (DAG) with random edges between variables\n",
    "        self.causal_graph = nx.DiGraph()\n",
    "        for v in self.variables:\n",
    "            self.causal_graph.add_node(v)\n",
    "\n",
    "    \n",
    "        adj_mat = np.random.choice([0,1], size=(num_vars, num_vars), p=[1-density,density])\n",
    "        adj_mat = np.tril(adj_mat, -1)\n",
    "        print(adj_mat)\n",
    "\n",
    "        for i in range(num_vars):\n",
    "            for j in range(num_vars):\n",
    "                if adj_mat[i][j] == 1:\n",
    "                    self.causal_graph.add_edge(i, j)\n",
    "\n",
    "        #for i in range(1, num_vars-1):\n",
    "        #    parent_indices = np.random.choice([j for j in range(i)], size=np.random.randint(0, i), replace=False)\n",
    "        #    for j in parent_indices:\n",
    "        #        self.causal_graph.add_edge(f'X{j}', f'X{i}')\n",
    "        #    self.causal_graph.add_edge(f'U{i}', f'X{i}')\n",
    "\n",
    "        # Check if the graph is acyclic\n",
    "        if not nx.is_directed_acyclic_graph(self.causal_graph):\n",
    "            raise ValueError(\"The generated causal graph contains cycles.\")\n",
    "\n",
    "        \n",
    "        # Define structural equations for variables and exogenous variables\n",
    "        for i, var in enumerate(self.variables):\n",
    "            parents = [p for p in self.causal_graph.predecessors(var)]\n",
    "            self.structural_equations[var] = (lambda *args: int(np.random.normal()), {p: None for p in parents})\n",
    "\n",
    "        for i, u in enumerate(self.exogenous_vars):\n",
    "            self.structural_equations[u] = (sample_from_normal, {})\n",
    "\n",
    "        self.scm = StructuralCausalModel(self.variables + self.exogenous_vars, self.structural_equations)\n",
    "\n",
    "        # Define the transition function based on the SCM and actions\n",
    "        def transition(state, action):\n",
    "            set_values = {}\n",
    "            for var, value in action:\n",
    "                set_values[var] = value\n",
    "            new_state = self.scm.get_sample(set_values)\n",
    "            return new_state\n",
    "\n",
    "        # Define the reward function (outside the SCM)\n",
    "        def reward(state):\n",
    "            R = sum(state.values())  # Example reward function (sum of variable values)\n",
    "            return R\n",
    "\n",
    "        self.transition = transition\n",
    "        self.reward = reward\n",
    "        self.T = T\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.scm.get_sample()\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        current_state = self.scm.values\n",
    "        next_state = self.transition(current_state, action)\n",
    "        reward = self.reward(next_state)\n",
    "        done = False  # Assuming episodes are not terminating\n",
    "        info = {}\n",
    "\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = 10\n",
    "num_actions = 3\n",
    "T = 10\n",
    "seed = 42\n",
    "env = CausalEnvironment(num_vars, num_actions, T, seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.draw(env.causal_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step([('X1',1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, epsilon_decay):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.num_actions = len(env.variables)\n",
    "        self.Q = {}  # Dictionary to store Q-values\n",
    "        \n",
    "    def update_Q(self, state, action, reward, next_state):\n",
    "        state_key = tuple(sorted(state.items()))\n",
    "        next_state_key = tuple(sorted(next_state.items()))\n",
    "\n",
    "        if state_key not in self.Q:\n",
    "            self.Q[state_key] = np.zeros((self.num_actions,))\n",
    "        if next_state_key not in self.Q:\n",
    "            self.Q[next_state_key] = np.zeros((self.num_actions,))\n",
    "\n",
    "        action_index = self.action_to_index(action)\n",
    "\n",
    "        old_q_value = self.Q[state_key][action_index]\n",
    "        max_next_q_value = np.max(self.Q[next_state_key])\n",
    "        new_q_value = old_q_value + self.alpha * (reward + self.gamma * max_next_q_value - old_q_value)\n",
    "\n",
    "        self.Q[state_key][action_index] = new_q_value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_key = tuple(sorted(state.items()))\n",
    "\n",
    "        if state_key not in self.Q or np.random.rand() < self.epsilon:\n",
    "            return self.index_to_action(np.random.randint(self.num_actions))  # Explore\n",
    "        else:\n",
    "            action_index = np.argmax(self.Q[state_key])\n",
    "            return self.index_to_action(action_index)  # Exploit\n",
    "\n",
    "    def action_to_index(self, action):\n",
    "        return action[1]\n",
    "\n",
    "    def index_to_action(self, index):\n",
    "        return ('X' + str(index), index)\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        cumulative_rewards = []\n",
    "        exploration_behavior = []\n",
    "\n",
    "        for episode in (range(num_episodes)):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            exploration_count = 0\n",
    "            \n",
    "            for _ in range(100):\n",
    "                action = self.choose_action(state)\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    exploration_count += 1\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step([action])\n",
    "                self.update_Q(state, action, reward, next_state)\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                \n",
    "\n",
    "            cumulative_rewards.append(episode_reward)\n",
    "            exploration_behavior.append(exploration_count)\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return cumulative_rewards, exploration_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment\n",
    "env = CausalEnvironment(num_vars=5, num_actions=5, T=10, seed=42)\n",
    "\n",
    "# Initialize the Q-learning agent\n",
    "agent = QLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.99)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 1000\n",
    "cumulative_rewards, exploration_behavior = agent.train(num_episodes)\n",
    "\n",
    "# Plot cumulative reward\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(range(num_episodes), cumulative_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward over Episodes')\n",
    "plt.show()\n",
    "\n",
    "# Plot exploration behavior\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_episodes), exploration_behavior)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Exploration Count')\n",
    "plt.title('Exploration Behavior over Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython -a\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cdt\n",
    "from cdt.metrics import precision_recall, SID, SHD\n",
    "from tqdm import tqdm\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.graph import SHD\n",
    "\n",
    "def shd_cl_metric(new_graph, graph):\n",
    "\n",
    "    metric = SHD.SHD(new_graph, graph)\n",
    "\n",
    "    return metric.get_shd()\n",
    "\n",
    "def pc_algo(history):\n",
    "    \n",
    "    #print('pc hsitory', history)\n",
    "    array_data = dicts_to_numpy_array(history)\n",
    "\n",
    "    \n",
    "    cg = pc(array_data, alpha=0.2, verbose=False)#, alpha=0.05, indep_test=\"chisq\")\n",
    "    \n",
    "\n",
    "    return cg.G\n",
    "\n",
    "class CausalCuriosityQLearningAgent:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, epsilon_decay, delta, beta):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.delta = delta\n",
    "        self.beta = beta\n",
    "        self.num_actions = len(env.variables)\n",
    "        \n",
    "        self.Q = {}  # Dictionary to store Q-values\n",
    "        self.D = np.zeros((self.num_actions,)) # Dictionary to store Distance value\n",
    "        self.causal_graph = None\n",
    "        self.history = []\n",
    "        self.history_distance = []\n",
    "        \n",
    "    def update_Q(self, state, action, reward, next_state):\n",
    "        state_key = tuple(sorted(state.items()))\n",
    "        next_state_key = tuple(sorted(next_state.items()))\n",
    "\n",
    "        if state_key not in self.Q:\n",
    "            self.Q[state_key] = np.zeros((self.num_actions,))\n",
    "        if next_state_key not in self.Q:\n",
    "            self.Q[next_state_key] = np.zeros((self.num_actions,))\n",
    "\n",
    "        action_index = self.action_to_index(action)\n",
    "\n",
    "        old_q_value = self.Q[state_key][action_index]\n",
    "        max_next_q_value = np.max(self.Q[next_state_key])\n",
    "        new_q_value = old_q_value + self.alpha * (reward + self.gamma * max_next_q_value - old_q_value)\n",
    "\n",
    "        self.Q[state_key][action_index] = new_q_value\n",
    "\n",
    "    def update_D(self, action, graph_distance):\n",
    "        \n",
    "        action_index = self.action_to_index(action)\n",
    "        if action_index not in self.D:\n",
    "            self.D[action_index] = 0\n",
    "        self.D[action_index] = self.delta * self.D[action_index] + self.beta * graph_distance\n",
    "        \n",
    "        #max_next_d_value = np.max(self.D)\n",
    "        #self.D[action_index] = self.D[action_index] + self.delta * (graph_distance + self.beta * max_next_d_value - self.D[action_index])\n",
    "\n",
    "    def update_causal_graph(self, state, action, causal_discovery_algo=pc_algo, graph_distance_metric=shd_cl_metric):\n",
    "        \n",
    "        new_causal_graph = causal_discovery_algo(self.history) \n",
    "\n",
    "        if self.causal_graph == None:\n",
    "            self.causal_graph = new_causal_graph\n",
    "        \n",
    "        graph_distance = graph_distance_metric(new_causal_graph, self.causal_graph) # un peu là aussi\n",
    "\n",
    "        return new_causal_graph, graph_distance\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_key = tuple(sorted(state.items()))\n",
    "\n",
    "        if state_key not in self.Q or np.random.rand() < self.epsilon:\n",
    "            return self.index_to_action(np.random.randint(self.num_actions))  # Explore\n",
    "        else:\n",
    "            action_index = np.argmax(self.Q[state_key])\n",
    "            #action_index = np.argmax(self.Q[state_key] + self.D)\n",
    "            return self.index_to_action(action_index)  # Exploit\n",
    "\n",
    "    def action_to_index(self, action):\n",
    "        \n",
    "        return action[1]\n",
    "\n",
    "    def index_to_action(self, index):\n",
    "        return ('X' + str(index), 1)\n",
    "\n",
    " \n",
    "\n",
    "    def train(self, num_episodes, causal_discovery_algo=pc_algo, graph_distance_metric=shd_cl_metric, flag=False):\n",
    "        \n",
    "        cumulative_rewards = []\n",
    "        exploration_behavior = []\n",
    "\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            exploration_count = 0\n",
    "            \n",
    "            for i in (range(100)):\n",
    "                action = self.choose_action(state)\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    exploration_count += 1\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step([action])\n",
    "                self.history.append(next_state)\n",
    "\n",
    "                int_reward = 0\n",
    "\n",
    "                if flag:\n",
    "                    if i > 10 and (i % 50 == 0):     \n",
    "                        new_causal_graph, graph_distance = self.update_causal_graph(state, action, causal_discovery_algo, graph_distance_metric)\n",
    "                    \n",
    "                        self.history_distance.append(graph_distance)\n",
    "                        #self.update_D(action, graph_distance)\n",
    "                        self.causal_graph = new_causal_graph\n",
    "                        #if not (np.argmax(self.Q) == 0): \n",
    "                        int_reward = self.beta*graph_distance \n",
    "                    else:\n",
    "                        int_reward = 0\n",
    "\n",
    "\n",
    "                self.update_Q(state, action, reward + int_reward, next_state)\n",
    "                state = next_state\n",
    "                episode_reward += reward \n",
    "                \n",
    "\n",
    "            cumulative_rewards.append(episode_reward)\n",
    "            exploration_behavior.append(exploration_count)\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        return cumulative_rewards, exploration_behavior\n",
    "\n",
    "    \n",
    "\n",
    "#def fci_algo(history):\n",
    "#\n",
    "#    array_data = dicts_to_numpy_array(history)\n",
    "#\n",
    "#    g, _ = fci(array_data)\n",
    "#\n",
    "#    return g\n",
    "\n",
    "def ges(history):\n",
    "\n",
    "    array_data = dicts_to_numpy_array(history)\n",
    "\n",
    "    record = ges(array_data)\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "def glasso_ges(history):\n",
    "    \n",
    "    glasso = cdt.independence.graph.Glasso()\n",
    "    print('Graph computation started...')\n",
    "    df_history = pd.DataFrame(history)\n",
    "    print('Glasso init...')\n",
    "    print('Glasso predict...')\n",
    "    skeleton = glasso.predict(df_history)\n",
    "    new_skeleton = cdt.utils.remove_indirect_links(skeleton, alg='aracne')\n",
    "    print('GES init...')\n",
    "    model = cdt.causality.graph.GES()\n",
    "    print('GES predict...')\n",
    "    #output_graph = model.predict(df_history, new_skeleton)\n",
    "    print('Done.')\n",
    "\n",
    "    return skeleton #output_graph\n",
    "\n",
    "def shd_metric(new_graph, graph):\n",
    "\n",
    "    return SHD(new_graph, graph)\n",
    "\n",
    "def dicts_to_numpy_array(dicts_list):\n",
    "    \"\"\"\n",
    "    Convertit une liste de dictionnaires en un tableau NumPy.\n",
    "    \n",
    "    :param dicts_list: Liste de dictionnaires avec les mêmes clés.\n",
    "    :return: Un tableau NumPy où chaque ligne correspond aux valeurs d'un dictionnaire.\n",
    "    \"\"\"\n",
    "    # Initialisation d'une liste vide pour stocker les tableaux NumPy intermédiaires\n",
    "    #print('in dicts to numpy')\n",
    "    \n",
    "    numpy_arrays_list = []\n",
    "    \n",
    "    for data_dict in dicts_list:\n",
    "        # Extraction des valeurs du dictionnaire courant\n",
    "        values = list(data_dict.values())\n",
    "        \n",
    "        # Conversion des valeurs en un tableau NumPy et ajout à la liste\n",
    "        numpy_arrays_list.append(np.array(values))\n",
    "    \n",
    "    # Concaténation de tous les tableaux NumPy intermédiaires en un seul tableau\n",
    "    # Utilise vstack pour empiler verticalement si la structure des données le requiert\n",
    "    combined_array = np.vstack(numpy_arrays_list)\n",
    "    \n",
    "    return combined_array\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineAgent(CausalCuriosityQLearningAgent):\n",
    "    def __init__(self, env, alpha=None, gamma=None, epsilon=1.0, epsilon_decay=0.99, delta=None, beta=None):\n",
    "        # Initialize the superclass with all the necessary arguments\n",
    "        super().__init__(env, alpha, gamma, epsilon, epsilon_decay, delta, beta)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_key = tuple(sorted(state.items()))\n",
    "\n",
    "        if state_key not in self.Q or np.random.rand() < self.epsilon:\n",
    "            # Explore\n",
    "            return self.index_to_action(np.random.randint(self.num_actions))\n",
    "        else:\n",
    "\n",
    "            # action_index = np.argmax(self.Q[state_key])\n",
    "\n",
    "            action_index = np.random.randint((self.num_actions))\n",
    "            return self.index_to_action(action_index)  # Exploit\n",
    "\n",
    "    def train(self, num_episodes, causal_discovery_algo=None, graph_distance_metric=None):\n",
    "\n",
    "        cumulative_rewards = []\n",
    "        exploration_behavior = []\n",
    "\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            exploration_count = 0\n",
    "\n",
    "            for i in (range(100)):\n",
    "                action = self.choose_action(state)\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    exploration_count += 1\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step([action])\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            cumulative_rewards.append(episode_reward)\n",
    "            exploration_behavior.append(exploration_count)\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return cumulative_rewards, exploration_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment\n",
    "env = CausalEnvironment(num_vars=5, num_actions=5, T=10, density=0.5, seed=42)\n",
    "\n",
    "# Initialize the Q-learning agent and causal Q learning agent\n",
    "agent = CausalCuriosityQLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.99, \n",
    "                                                delta=0.2, beta = 0.1)\n",
    "\n",
    "#agent = QLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.99)\n",
    "agent_causal = CausalCuriosityQLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.99, \n",
    "                                                delta=0.2, beta = 0.1)\n",
    "baseline = BaselineAgent(env)           \n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 1000\n",
    "cumulative_rewards_causal, exploration_behavior_causal = agent_causal.train(num_episodes, pc_algo, shd_cl_metric, flag=True)\n",
    "cumulative_rewards, _ = agent.train(num_episodes, flag=False)\n",
    "cumulative_rewards_random, _ = agent.train(num_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative reward\n",
    "#.figure(figsize=(10, 6))\n",
    "#.plot(range(num_episodes), cumulative_rewards_causal)\n",
    "#.xlabel('Episode')\n",
    "#.ylabel('Cumulative Reward')\n",
    "#.title('Cumulative Reward over Episodes')\n",
    "#.show()\n",
    "\n",
    "# Plot exploration behavior\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.plot(range(num_episodes), exploration_behavior_causal)\n",
    "#plt.xlabel('Episode')\n",
    "#plt.ylabel('Exploration Count')\n",
    "#plt.title('Exploration Behavior over Episodes')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting cumulative rewards\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(cumulative_rewards, label='Traditional Q-Learning')\n",
    "plt.plot(cumulative_rewards_causal, label='Causal Curiosity Q-Learning')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Comparison of Cumulative Rewards')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "# Perform the T-test\n",
    "t_stat, p_value = ttest_ind(cumulative_rewards, cumulative_rewards_causal)\n",
    "\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython -a\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of values for beta and delta to be tested\n",
    "beta_values = np.linspace(start=0.1, stop=1.0, num=10)  # Example range for beta\n",
    "delta_values = np.linspace(start=0.1, stop=1.0, num=10)  # Example range for delta\n",
    "\n",
    "performance_results = np.zeros((len(beta_values), len(delta_values)))\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "for i, beta in (enumerate(beta_values)):\n",
    "    for j, delta in enumerate(delta_values):\n",
    "        # Initialize the agent with current beta and delta\n",
    "        agent_causal = CausalCuriosityQLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=1.0, \n",
    "                                                     epsilon_decay=0.99, delta=delta, beta=beta)\n",
    "        \n",
    "        # Train the agent and get performance metric\n",
    "        cumulative_reward, _ = agent_causal.train(num_episodes, pc_algo, shd_cl_metric)\n",
    "        \n",
    "        # Store the cumulative reward or other performance metric\n",
    "        performance_results[i, j] = cumulative_reward[-1] #np.mean(cumulative_reward)  # Example: using mean cumulative reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a meshgrid for plotting\n",
    "B, D = np.meshgrid(beta_values, delta_values)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "cp = plt.contourf(B, D, performance_results, 20, cmap='viridis')  # Adjust colormap as needed\n",
    "plt.colorbar(cp)\n",
    "plt.title('Agent Performance Across Beta and Delta Values')\n",
    "plt.xlabel('Beta (Curiosity Reward Parameter)')\n",
    "plt.ylabel('Delta (Curiosity Decay Rate)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython -a\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of values for beta and delta to be tested\n",
    "beta_values = np.linspace(start=0.0, stop=1.0, num=10)  # Example range for beta\n",
    "\n",
    "delta_values = np.linspace(start=0.0, stop=1.0, num=10)  # Example range for delta\n",
    "\n",
    "performance_results = np.zeros((len(beta_values),))# len(delta_values)))\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "for i, beta in (enumerate(beta_values)):\n",
    "   \n",
    "        # Initialize the agent with current beta and delta\n",
    "    agent_causal = CausalCuriosityQLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=1.0, \n",
    "                                                     epsilon_decay=0.99, delta=0.0, beta=beta)\n",
    "    \n",
    "    # Train the agent and get performance metric\n",
    "    cumulative_reward, _ = agent_causal.train(num_episodes, pc_algo, shd_cl_metric, flag=True)\n",
    "    \n",
    "    # Store the cumulative reward or other performance metric\n",
    "    performance_results[i] = np.mean(cumulative_reward)  # Example: using mean cumulative reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Agent Performance Across Beta Values')\n",
    "plt.plot(beta_values, performance_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shd_metric(agent_causal.causal_graph, agent_causal.causal_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent_causal.history_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use Pandas to create a rolling window object\n",
    "window_size = 10\n",
    "rolling_window = pd.Series(agent_causal.history_distance).rolling(window=window_size)\n",
    "\n",
    "# Step 4: Calculate the rolling average\n",
    "rolling_average = rolling_window.mean()\n",
    "\n",
    "# Step 5: Plot the original data along with the rolling average\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(agent_causal.history_distance, label='Original Data', color='blue')\n",
    "plt.plot(rolling_average, label=f'Rolling Average (Window Size: {window_size})', color='red')\n",
    "plt.legend()\n",
    "plt.title('Rolling Average of graph distance between succesive models')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineAgent(CausalCuriosityQLearningAgent):\n",
    "    def __init__(self, env, alpha=None, gamma=None, epsilon=1.0, epsilon_decay=0.99, delta=None, beta=None):\n",
    "        # Initialize the superclass with all the necessary arguments\n",
    "        super().__init__(env, alpha, gamma, epsilon, epsilon_decay, delta, beta)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_key = tuple(sorted(state.items()))\n",
    "\n",
    "        if state_key not in self.Q or np.random.rand() < self.epsilon:\n",
    "            # Explore\n",
    "            return self.index_to_action(np.random.randint(self.num_actions))\n",
    "        else:\n",
    "\n",
    "            # action_index = np.argmax(self.Q[state_key])\n",
    "\n",
    "            action_index = np.random.randint((self.num_actions))\n",
    "            return self.index_to_action(action_index)  # Exploit\n",
    "\n",
    "    def train(self, num_episodes, causal_discovery_algo=None, graph_distance_metric=None):\n",
    "\n",
    "        cumulative_rewards = []\n",
    "        exploration_behavior = []\n",
    "\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            exploration_count = 0\n",
    "\n",
    "            for i in (range(100)):\n",
    "                action = self.choose_action(state)\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    exploration_count += 1\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step([action])\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            cumulative_rewards.append(episode_reward)\n",
    "            exploration_behavior.append(exploration_count)\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return cumulative_rewards, exploration_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = BaselineAgent(env)\n",
    "cumulative_rewards_random, _ = random_agent.train(num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting cumulative rewards\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(cumulative_rewards_random, label='Baseline')\n",
    "plt.plot(cumulative_rewards, label='Traditional Q-Learning')\n",
    "plt.plot(cumulative_rewards_causal, label='Causal Curiosity Q-Learning')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Comparison of Cumulative Rewards')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "# Perform the T-test\n",
    "t_stat, p_value = ttest_ind(cumulative_rewards_random, cumulative_rewards_causal)\n",
    "\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdt\n",
    "import pandas as pd\n",
    "glasso = cdt.independence.graph.Glasso()\n",
    "data = [\n",
    "    {'A': 1, 'B': 2, 'C': 3},\n",
    "    {'A': 4, 'B': 5, 'C': 6},\n",
    "    {'A': 7, 'B': 8, 'C': 9}\n",
    "]\n",
    "df_history = pd.DataFrame(array_of_dicts)\n",
    "skeleton = glasso.predict(df_history)\n",
    "print(skeleton)\n",
    "new_skeleton = cdt.utils.remove_indirect_links(skeleton, alg='aracne')\n",
    "cdt.SETTINGS.rpath = \"/usr/local/bin/R\"\n",
    "\n",
    "#model = cdt.causality.graph.GES()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install causal-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causallearn.search.ConstraintBased.PC import pc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Extracting keys to define the order of features\n",
    "features = list(data[0].keys())\n",
    "\n",
    "# Creating an empty NumPy array to hold the data\n",
    "num_samples = len(data)\n",
    "num_features = len(features)\n",
    "array_data = np.zeros((num_samples, num_features))\n",
    "\n",
    "# Filling in the array with the data from dictionaries\n",
    "for i, sample in enumerate(data):\n",
    "    for j, feature in enumerate(features):\n",
    "        array_data[i, j] = sample[feature]\n",
    "\n",
    "print(array_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = pc(array_data, alpha=0.05, indep_test=\"chisq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg.to_nx_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg.draw_nx_graph(skel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causallearn.graph import SHD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = SHD.SHD(cg.G, cg.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.get_shd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dicts_to_numpy_array(dicts_list):\n",
    "    \"\"\"\n",
    "    Convertit une liste de dictionnaires en un tableau NumPy.\n",
    "    \n",
    "    :param dicts_list: Liste de dictionnaires avec les mêmes clés.\n",
    "    :return: Un tableau NumPy où chaque ligne correspond aux valeurs d'un dictionnaire.\n",
    "    \"\"\"\n",
    "    # Initialisation d'une liste vide pour stocker les tableaux NumPy intermédiaires\n",
    "    print('in dicts to numpy')\n",
    "    numpy_arrays_list = []\n",
    "    \n",
    "    for data_dict in dicts_list:\n",
    "        # Extraction des valeurs du dictionnaire courant\n",
    "        values = list(data_dict.values())\n",
    "        \n",
    "        # Conversion des valeurs en un tableau NumPy et ajout à la liste\n",
    "        numpy_arrays_list.append(np.array(values))\n",
    "    \n",
    "    # Concaténation de tous les tableaux NumPy intermédiaires en un seul tableau\n",
    "    # Utilise vstack pour empiler verticalement si la structure des données le requiert\n",
    "    combined_array = np.vstack(numpy_arrays_list)\n",
    "    \n",
    "    return combined_array\n",
    "\n",
    "# Exemple d'utilisation\n",
    "dicts_list = [\n",
    "    {'X0': 2, 'X1': 1},  # Remplacez ... par le reste des clés et valeurs\n",
    "    {'X0': 3, 'X1': 0},  # Second dictionnaire, etc.\n",
    "    # Ajoutez d'autres dictionnaires selon le besoin\n",
    "]\n",
    "\n",
    "result_array = dicts_to_numpy_array(dicts_list)\n",
    "print(result_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
